[
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "",
    "text": "In recent years, we saw tremendous progress across all machine learning tasks from image classification, object detection, and translation, to question answering, text classification, and more. Main drivers of this progress are more compute resources, data, new training approaches such as transfer learning, and also more sophisticated neural network architectures. Fortunately these models are available at our fingertips. If you tackle a computer vision task, you can leverage the Pytorch Image Models (timm) library. As of now it contains more than 50 different types of architectures translating into 600 models which have different trade-offs in terms of accuracy and inference time, depending on the task. For NLP, you can use the popular transformer library from HuggingFace which offers almost 80k models based on about 140 different types of architectures.\nEach of these neural network architectures, built upon each other by the research community, were able to achieve better performance thanks to very carefully hand-crafted inductive bias for each specific task. For example, CNNs used in computer vision models such as ResNet have a locality bias where they assume close pixels are related to each other. LSTMs, which used to be popular for NLP tasks before Transformers appeared, have a sequential bias where they process each element of a sequence one after another. However, because the inductive bias is hand-crafted for a specific task, existing architectures are often not able to generalize across multiple modalities. The Perceiver IO (Jaegle et al, 2021) architecture released by DeepMind in 2021 aims to solve this challenge and demonstrate they can achieve strong results across a wide variety of single-modal and multi-modal tasks such as: GLUE language benchmark, predicting optical flow between images, image classification, multi-modal video + audio classification, audio-video-label multi-modal autoencoding, etc.\nIn this blog post, we will implement a Perceiver IO architecture from scratch and we will apply it to two different domains: image classification and text classification. In conjunction to this blog post, I highly recommend watching the main author of the Perceiver IO paper Drew Jaegle’s talk, and reading this HuggingFace blog post. These resources, in addition to training several Perceiver models with the transformers library, inspired me to implement this architecture from scratch."
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#transformers",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#transformers",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Transformers",
    "text": "Transformers\nThe Perceiver architecture is based on the Transformer architecture (Vaswani et al., 2017). In this presentation, Drew Jaegle describes why transformers are a good candidate for general purpose architectures:\n\nThey don’t make specific assumptions about the domain.\nThey have general purpose inductive bias.\nPosition is a feature instead of being a constraint.\nThey mainly use matmuls, which are efficiently computed by GPU and TPU.\n\nThe cons are:\n\nAttention scales quadratically based on the input.\nMultilayer perceptron (MLP) scales linearly based on the input.\n\nTransformers consist of: a self-attention module, multilayer perceptron (also called feed-forward layer), layer normalization, and skip connections. But really the heart of the transformers is their self-attention module, which the Perceiver architecture innovatively modified to make the module scalable. So let’s first implement the self-attention module, understand why it scales quadratically, and learn how the Perceiver architecture addresses this challenge.\n\n\n\n\n\nFigure 1: Self-Attention with positional encodig from Attention Is All You Need (Vaswani et al 2017)\n\n\n\nSelf-Attention\nTo build up some intuition about the self-attention mechanism, I highly recommend Peter Bloem’s blog post or the natural language processing with transformers book. Later in this post, we will demonstrate how the perceiver can be applied to images. But let’s assume for now that our input is a sentence where each word of the sentence is represented by token embeddings.\nThe self-attention mechanism consists of the following steps:\n\nProject the input (token embeddings in our case) into a query, key and value.\nCompute the similarity between the query and the key using a dot product. The result of the dot product represents the attention scores.\nMultiply the attention scores by a scaling factor to normalize their variance and apply a softmax so each column of the matrix, representing the attention weights, sums up to 1.\nCompute the dot product between the attention weights and the values.\n\nThe main purpose of the self-attention mechanism is to produce a new representation of the token embeddings where now each new embeddings is a linear combination (or weighted average) of the original embeddings. This allows us to encode some contextual information. For example, we have the following sentence: “The tree bark is rough to the touch” and “I hope her dog doesn’t bark when I knock on the door.” In these two sentences, we have the word “bark” but each has a different meaning. In the first sentence, “bark” refers to the tree, and in the second sentence it refers to the dog. When applying the self-attention mechanism to the embedding “bark” the new embedding should give more weight to the word “tree” in the first sentence and “dog” in the second one in order to encode this context.\n\nclass SelfAttention(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        n_channels,\n    ):\n        super().__init__()\n        self.q = nn.Linear(input_dim, n_channels)\n        self.k = nn.Linear(input_dim, n_channels)\n        self.v = nn.Linear(input_dim, n_channels)\n\n    def forward(self, input):\n        # (N, input_dim) . (input_dim, qk_channels) -> (N, qk_channels)\n        query = self.q(input)\n        # (N, input_dim) . (input_dim, qk_channels) -> (N, qk_channels)\n        key = self.k(input)\n        # (N, input_dim) . (input_dim, v_channels) -> (N, v_channels)\n        value = self.v(input)\n\n        scale = 1.0 / math.sqrt(query.size(-1))\n        # (N, qk_channels) . (qk_channels, N) -> (N, N)\n        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n        print(f\"Attention score shape: {scores.shape}\")\n        weights = F.softmax(scores, dim=-1)\n        # (N, N) . (N, v_channels) -> (N, v_channels)\n        return torch.bmm(weights, value)\n\nLet’s apply the self-attention module to a tensor of shape (1, 3, 5). We will assume the input is a sentence containing three words, each represented by an embedding token of length 5.\n\nx_embed = torch.ones(1, 3, 5)\nprint(f\"Input shape: {x_embed.shape}\")\nself_attn = SelfAttention(5, 3)\nattn_out = self_attn(x_embed)\nprint(f\"Output shape {attn_out.shape}\")\n\nInput shape: torch.Size([1, 3, 5])\nAttention score shape: torch.Size([1, 3, 3])\nOutput shape torch.Size([1, 3, 3])\n\n\nWe can see the attention score has a shape of (1, 3, 3) and the output shape is (1, 3, 5). Let’s now apply the self-attention mechanism to an input containing 10 words and see how it impacts the size of attention score tensor and the output.\n\nx = torch.ones(1, 10, 5)\nprint(f\"Input shape: {x.shape}\")\nself_attn = SelfAttention(5, 3)\nattn_out = self_attn(x)\nprint(f\"Output shape {attn_out.shape}\")\n\nInput shape: torch.Size([1, 10, 5])\nAttention score shape: torch.Size([1, 10, 10])\nOutput shape torch.Size([1, 10, 3])\n\n\nFor an input of shape (1, 10, 5), the new attention score shape is (1, 10, 10). So as you can see the attention score scales quadratically based on the input. Same thing happens for the second dot product between the weights and the values, and the output shape is (1, 10, 3). Therefore it increases linearly. Later we will see that the output of the self-attention module is fed into an MLP layer. Because the output of the self-attention module scales linearly based on the input, the MLP layer will also scale linearly based on the input.\nOk, so let’s learn how Perceiver IO is addressing the transformer scalability issue while maintaining its generality property.\n\n\nCross-Attention\nPerceiver IO introduces the idea of using a cross attention module to encode the input into a latent space. To do so, instead of passing the input to the query, Perceiver IO uses a latent variable, which has smaller dimension than the input. You can think about this latent variable as the learned initial stated for RNN.\n\n\n\n\n\nFigure 2: cross-attention from Perceiver IO encoder from (Jaegle et al 2022)\n\n\nYou can find other examples of cross-attention for computer vision in the papers: End-to-End Object Detection with Transformers (Carion et al, 2020), and Object-Centric Learning with Slot Attention (Locatello et al, 2020).\n\nclass CrossAttention(nn.Module):\n    def __init__(\n        self,\n        input_kv_dim,\n        input_q_dim,\n        qk_channels,\n        v_channels,\n    ):\n        super().__init__()\n        self.q = nn.Linear(input_q_dim, qk_channels)\n        self.k = nn.Linear(input_kv_dim, qk_channels)\n        self.v = nn.Linear(input_kv_dim, v_channels)\n\n    def forward(self, input_kv, input_q):\n        # (M, input_q_dim) . (input_q_dim, qk_channels) -> (N, qk_channels)\n        query = self.q(input_q)\n        # (N, input_kv_dim) . (input_kv_dim, qk_channels) -> (N, qk_channels)\n        key = self.k(input_kv)\n        # (N, input_kv_dim) . (input_kv_dim, v_channels) -> (N, v_channels)\n        value = self.v(input_kv)\n\n        scale = 1.0 / math.sqrt(query.size(-1))\n        # (M, qk_channels) . (qk_channels, N) -> (M, N)\n        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n        print(f\"Attention score shape: {scores.shape}\")\n        weights = F.softmax(scores, dim=-1)\n        # (M, N) . (N, v_channels) -> (M, v_channels)\n        return torch.bmm(weights, value)\n\nTo demonstrate it’s cheaper to encode the input to a latent space instead of using self-attention, let’s go through concrete examples.\n\nx = torch.ones(1, 10, 5)\nlatent = nn.Parameter(torch.randn(1, 2, 3))\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Latent shape: {latent.shape}\")\nself_attn = CrossAttention(5, 3, 4, 3)\nattn_out = self_attn(x, latent)\nprint(f\"Output shape {attn_out.shape}\")\n\nInput shape: torch.Size([1, 10, 5])\nLatent shape: torch.Size([1, 2, 3])\nAttention score shape: torch.Size([1, 2, 10])\nOutput shape torch.Size([1, 2, 3])\n\n\nLet’s say we have our previous input of shape (1, 10, 5) and a new latent variable of shape (1, 2, 3). The attention score now has a shape of (1, 2, 10). So instead of scaling quadratically based on the input size, the score scales linearly based on the size of the latent variable which can be controlled. The second matrix of the cross-attention module also scales linearly. But more importantly, the output, which the MLP layer will use, also scales linearly based on the latent size instead of the input size. The output now has a shape of (1, 2, 3) instead of (1, 10, 3).\nExcellent! We now have implemented a cross-attention module which scales linearly based on the latent variable, which will allow us to compute on larger inputs such as images or longer text sequences. Later, you will see that the Perceiver IO architecture also uses a self-attention module on the latent array. So let’s create a general attention module which can be parametrized to become a self-attention or cross-attention module depending on our need.\n\nclass AttentionHead(nn.Module):\n    def __init__(\n        self,\n        is_cross_attention,\n        input_kv_dim,\n        input_q_dim,\n        qk_channels_per_head,\n        v_channels_per_head,\n        attention_prob_dropout_prob=0.1,\n    ):\n        super().__init__()\n        self.is_cross_attention = is_cross_attention\n        if not is_cross_attention:\n            input_kv_dim = input_q_dim\n\n        self.q = nn.Linear(input_q_dim, qk_channels_per_head)\n        self.k = nn.Linear(input_kv_dim, qk_channels_per_head)\n        self.v = nn.Linear(input_kv_dim, v_channels_per_head)\n        self.dropout = nn.Dropout(attention_prob_dropout_prob)\n\n    def forward(self, input_kv, input_q):\n        query = self.q(input_q)\n\n        if self.is_cross_attention & (input_kv is not None):\n            key = self.k(input_kv)\n            value = self.v(input_kv)\n        else:\n            key = self.k(input_q)\n            value = self.v(input_q)\n\n        scale = 1.0 / math.sqrt(query.size(-1))\n        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n        weights = F.softmax(scores, dim=-1)\n        weights = self.dropout(weights)\n        return torch.bmm(weights, value)\n\nUntil now, we have implemented a single head attention, but most transformers are multi-head attention. The benefits of having multiple heads are that each head can focus on different aspects of an image (edges, colors, etc.) or a sentence, instead of on a single aspect.\nWe can simply create a multi-head attention layer by instantiating several heads and concatenating their outputs. Usually we also apply a linear layer to the final output. Note that it’s possible to avoid instantiating an AttentionHead for each head and concatenating their outputs. You could have the linear layers with number of output features equal to number of channels per head * number of heads for the query, key and value. Then you could reshape the output from the query, key and value to (batch_size, num_heads, time (N or M), number of channels per head). As an example, you can check the transformers implementation.\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(\n        self,\n        is_cross_attention,\n        input_kv_dim,\n        input_q_dim,\n        qk_channels,\n        v_channels,\n        num_heads,\n    ):\n        super().__init__()\n\n        qk_channels_per_head = qk_channels // num_heads\n        v_channels_per_head = v_channels // num_heads\n\n        self.heads = nn.ModuleList(\n            [\n                AttentionHead(\n                    is_cross_attention,\n                    input_kv_dim,\n                    input_q_dim,\n                    qk_channels_per_head,\n                    v_channels_per_head,\n                )\n                for _ in range(num_heads)\n            ]\n        )\n\n        self.linear = nn.Linear(v_channels, v_channels)\n\n    def forward(self, input, latent_embedding):\n        x = torch.cat([h(input, latent_embedding) for h in self.heads], dim=-1)\n        return self.linear(x)\n\n\nself_attention = MultiHeadAttention(True, 5, 3, 8, 6, 2)\nself_attention_multi_head = self_attention(x, latent)\nprint(f\"Multi-head attention input shape: {x.shape}\")\nprint(f\"Multi-head attention output shape: {self_attention_multi_head.shape}\")\n\nMulti-head attention input shape: torch.Size([1, 10, 5])\nMulti-head attention output shape: torch.Size([1, 2, 6])\n\n\nVoilà, we have our multi-head attention layer which can become a self-attention or cross-attention layer by setting the is_cross_attention parameter. Since we have implemented the main building block, we are ready to implement the full Perceiver architecture!"
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#perceiver-architecture",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#perceiver-architecture",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Perceiver Architecture",
    "text": "Perceiver Architecture\nThe Perceiver architecture consists of three main build blocks: encoder, processor and decoder. The input gets first encoded into a latent array, then the latent representation gets refined via several processing layers. Finally, the latent gets decoded into an output. As you can see in the diagram below, the encoder and decoder are using a cross-attention module, and the processor is using a self-attention module. What’s amazing with the Perceiver architecture is that it can handle any modalities thanks to the encoder and decoder modules. Additionally, the size of the inputs and outputs is not a problem anymore since they both use a cross-attention module where the latent size is independent of the input size.\n\n\n\n\nFigure 3: encode (cross-attention), process (self-attention) and decode (cross-attention) attention modules from Perceiver IO (Jaegle et al 2022)\n\n\nLet’s implement the encoder!\n\nEncoder\nThe Perceiver encoder module is very similar to the encoder you could find in Bert architecture except it uses cross-attention with a latent variable. In addition to the cross-attention module we just need to add:\n\nA multi-perceptron module: two fully connected layers processing each latent vector independently, a GELU activation, and a dropout layer.\nTwo layer-normalization layers\n\nTwo skip connections\n\nOk, let’s see how we can combine these components to build our encoder.\n\nclass MLP(nn.Module):\n    def __init__(self, input_size, widening_factor, dropout_prob=0.0):\n        super().__init__()\n        self.dense1 = nn.Linear(input_size, input_size * widening_factor)\n        self.dense2 = nn.Linear(input_size * widening_factor, input_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout()\n\n    def forward(self, x):\n        x = self.dense1(x)\n        x = self.gelu(x)\n        x = self.dense2(x)\n        return self.dropout(x)\n\n\nclass PerceiverEncoder(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        latent_embedding_dim,\n        qk_channels,\n        v_channels,\n        num_heads,\n        widening_factor,\n    ):\n        super().__init__()\n\n        self.layer_norm_1 = nn.LayerNorm(input_dim)\n        self.layer_norm_2 = nn.LayerNorm(latent_embedding_dim)\n\n        self.attention = MultiHeadAttention(\n            is_cross_attention=True,\n            input_kv_dim=input_dim,\n            input_q_dim=latent_embedding_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n        )\n\n        self.mlp = MLP(v_channels, widening_factor=widening_factor)\n\n    def forward(self, input, latent):\n        input_norm = self.layer_norm_1(input)\n        latent_embedding_norm = self.layer_norm_2(latent)\n        x_qkv = self.attention(input_norm, latent_embedding_norm)\n        x_qkv = x_qkv + latent\n        x_qkv = x_qkv + self.mlp(latent_embedding_norm)\n        return x_qkv\n\n\nperceiver_encoded = PerceiverEncoder(6, 6, 8, 6, 2, 2)\ninput = torch.ones(size=(1, 3, 6))\nlatent = nn.Parameter(torch.randn(1, 2, 6))\nencoder_output = perceiver_encoded(input, latent)\nprint(f\"Input shape: {input.shape}\")\nprint(f\"Latent variable shape: {latent.shape}\")\nprint(f\"Encoder output shape: {encoder_output.shape}\")\n\nInput shape: torch.Size([1, 3, 6])\nLatent variable shape: torch.Size([1, 2, 6])\nEncoder output shape: torch.Size([1, 2, 6])\n\n\nExcellent, we have successfully implemented our encoder layer. In fact, the processor and the decoder use the exact same ingredients, except the processor will use a self-attention module. So let’s refactor our encoder to use PerceiverLayer which can be parametrized to use a cross-attention or self-attention module.\n\nclass PerceiverLayer(nn.Module):\n    def __init__(\n        self,\n        input_kv_dim,\n        input_q_dim,\n        qk_channels,\n        v_channels,\n        num_heads,\n        widening_factor,\n        is_cross_attention,\n    ):\n        super().__init__()\n\n        if input_kv_dim is None:\n            input_kv_dim = input_q_dim\n\n        self.layer_norm_1 = nn.LayerNorm(input_kv_dim)\n        self.layer_norm_2 = nn.LayerNorm(input_q_dim)\n\n        self.attention = MultiHeadAttention(\n            is_cross_attention=is_cross_attention,\n            input_kv_dim=input_kv_dim,\n            input_q_dim=input_q_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n        )\n\n        self.mlp = MLP(v_channels, widening_factor=widening_factor)\n\n    def forward(self, input_kv, input_q):\n        input_kv_norm = self.layer_norm_1(input_kv)\n        input_q_norm = self.layer_norm_2(input_q)\n        x_qkv = self.attention(input_kv_norm, input_q_norm)\n        x_qkv = x_qkv + input_q\n        x_qkv = x_qkv + self.mlp(input_q_norm)\n        return x_qkv\n\n\nclass PerceiverEncoder(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        latent_dim,\n        qk_channels,\n        v_channels,\n        num_heads,\n        widening_factor,\n    ):\n        super().__init__()\n\n        self.encoder = PerceiverLayer(\n            input_kv_dim=input_dim,\n            input_q_dim=latent_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n            widening_factor=widening_factor,\n            is_cross_attention=True,\n        )\n\n    def forward(self, input, latent_embeddings):\n        return self.encoder(input_kv=input, input_q=latent_embeddings)\n\n\nperceiver_encoded = PerceiverEncoder(6, 6, 8, 6, 2, 2)\ninput = torch.ones(size=(1, 3, 6))\nlatent = nn.Parameter(torch.randn(1, 2, 6))\nperceiver_encoded(input, latent).shape\n\nencoder_output = perceiver_encoded(input, latent)\nprint(f\"Input shape: {input.shape}\")\nprint(f\"Latent variable shape: {latent.shape}\")\nprint(f\"Encoder output shape: {encoder_output.shape}\")\n\nInput shape: torch.Size([1, 3, 6])\nLatent variable shape: torch.Size([1, 2, 6])\nEncoder output shape: torch.Size([1, 2, 6])\n\n\nOur PerceiverEncoder layer now looks much simpler with our new generic PerceiverLayer.\n\n\nProcessor\nOur Processor layer is going to be responsible for refining the latent representation we obtained from the encoder. So this layer has a single input, the latent array, and we apply a self-attention module in conjunction with an MLP layer, layer-normalization, and skip connection (i.e PerceiverLayer configured as self-attention).\n\nclass PerceiverProcessor(nn.Module):\n    def __init__(self, latent_dim, qk_channels, v_channels, num_heads, widening_factor):\n        super().__init__()\n\n        self.processor = PerceiverLayer(\n            input_kv_dim=None,\n            input_q_dim=latent_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n            widening_factor=widening_factor,\n            is_cross_attention=False,\n        )\n\n    def forward(self, latent):\n        return self.processor(input_kv=latent, input_q=latent)\n\n\nlatent = nn.Parameter(torch.randn(1, 2, 6))\nperceiver_processor = PerceiverProcessor(6, 8, 6, 2, 2)\nprocessor_output = perceiver_processor(latent)\n\nprint(f\"Latent array (input) shape: {latent.shape}\")\nprint(f\"Processor output shape: {processor_output.shape}\")\n\nLatent array (input) shape: torch.Size([1, 2, 6])\nProcessor output shape: torch.Size([1, 2, 6])\n\n\nWe are almost ready to combine everything to create a complete architecture. We just need to implement the decoder module.\n\n\nDecoder\nThe perceiver decoder will map the latent array to an output array. To do so, we can query the latent array with a query vector. Note that this query vector can be hand-designed, or learned embeddings, or a function of the input. For this blog post, we will use learned embeddings. To query the latent array returned by the processor, we simply need to compute the cross-attention between a learned query variable and the latent array. The query should have the same number of elements as the desired output.\n\nclass PerceiverDecoder(nn.Module):\n    def __init__(\n        self,\n        num_output_channels,\n        latent_dim,\n        query_dim,\n        qk_channels,\n        v_channels,\n        num_heads,\n        widening_factor,\n    ):\n        super().__init__()\n\n        self.decoder = PerceiverLayer(\n            input_kv_dim=latent_dim,\n            input_q_dim=query_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n            widening_factor=widening_factor,\n            is_cross_attention=True,\n        )\n\n        self.dense = nn.Linear(query_dim, num_output_channels)\n\n    def forward(self, latent, query):\n        attn_output = self.decoder(latent, query)\n        logit = self.dense(attn_output)\n        return logit\n\nLet’s run the decoder with an expected output of 10 elements. For example, an image classification task with 10 potential different labels.\n\nlatent = torch.ones(1, 3, 5)\nquery_variable = nn.Parameter(torch.randn(1, 1, 10))\nq_dim = 10\nkv_dim = 5\nnum_output_channels = 10\nqk_channels = q_dim\nv_channels = qk_channels\nnum_heads = 1\nwidening_factor = 1\n\nperceiver_decoder = PerceiverDecoder(\n    num_output_channels,\n    kv_dim,\n    q_dim,\n    qk_channels,\n    v_channels,\n    num_heads,\n    widening_factor,\n)\n\nperceiver_decoder_output = perceiver_decoder(latent, query_variable)\nprint(f\"Latent array shape: {latent.shape}\")\nprint(f\"Query variable shape: {query_variable.shape}\")\nprint(f\"Perceiver decoder output shape: {perceiver_decoder_output.shape}\")\n\nLatent array shape: torch.Size([1, 3, 5])\nQuery variable shape: torch.Size([1, 1, 10])\nPerceiver decoder output shape: torch.Size([1, 1, 10])\n\n\nAs expected, the output of the decoder returns 10 elements matching the number of labels.\n\n\nPerceiver IO = Encoder + Processor + Decoder\nWe now have all the building blocks to create a complete PerceiverIO architecture. As discussed earlier, the latent variable for the encoder and the query variable for the decoder are learned. So we can create a LearnedEmbedding layer to instantiate latent and query variables. Otherwise, it’s straight-forward. The encoder takes the input array as an input, and the latent variable then returns a latent array. Then this latent array is fed into the processor. Finally the latent array is queried by the decoder with a learned query. The output of our decoder will be the logits, which we will use to compute our loss when training the image and text classification task.\n\nclass LearnedEmbeddings(nn.Module):\n    def __init__(self, index_dim, num_channels=128):\n        super().__init__()\n        self.index_dim = index_dim\n        self.num_channels = num_channels\n        self.learned_embeddings = nn.Parameter(torch.randn(index_dim, num_channels))\n\n    def forward(self, batch_size):\n        return self.learned_embeddings.expand(batch_size, -1, -1)\n\n\nclass PerceiverIO(nn.Module):\n    def __init__(\n        self,\n        n_labels,\n        input_dim,\n        latent_embedding_dim,\n        query_dim,\n        qk_channels,\n        v_channels,\n        num_latents,\n        num_heads,\n        nb_processor,\n        widening_factor,\n        input_processor=None,\n    ):\n        super().__init__()\n\n        self.input_processor = input_processor if input_processor else nn.Identity()\n\n        self.latent_embeddings = LearnedEmbeddings(num_latents, latent_embedding_dim)\n        self.query_embeddings = LearnedEmbeddings(1, query_dim)\n\n        self.encoder = PerceiverEncoder(\n            input_dim=input_dim,\n            latent_dim=latent_embedding_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n            widening_factor=widening_factor,\n        )\n\n        self.processors = nn.ModuleList(\n            [\n                PerceiverProcessor(\n                    latent_dim=latent_embedding_dim,\n                    qk_channels=qk_channels,\n                    v_channels=v_channels,\n                    num_heads=num_heads,\n                    widening_factor=widening_factor,\n                )\n                for _ in range(nb_processor)\n            ]\n        )\n\n        self.decoder = PerceiverDecoder(\n            num_output_channels=n_labels,\n            latent_dim=latent_embedding_dim,\n            query_dim=query_dim,\n            qk_channels=qk_channels,\n            v_channels=v_channels,\n            num_heads=num_heads,\n            widening_factor=widening_factor,\n        )\n\n    def forward(self, inputs):\n        batch_size = inputs.shape[0]\n\n        latent_embeddings = self.latent_embeddings(batch_size)\n        query_embeddings = self.query_embeddings(batch_size)\n\n        inputs = self.input_processor(inputs)\n        x = self.encoder(inputs, latent_embeddings)\n\n        for p in self.processors:\n            x = p(x)\n\n        logits = self.decoder(x, query_embeddings)\n\n        return logits[:, 0, :]\n\nVoilà, we have now our full Perceiver IO architecture. Let’s now dive into how in the authors of the Perceiver IO paper pre-process an image or text to obtain the input arrays which will be fed into the encoder."
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#image-classification",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#image-classification",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Image Classification",
    "text": "Image Classification\n\nPre-Processing\nTo pre-process an image, the authors of the Perceiver IO paper use different techniques involving 2D Fourier position embeddings with 2D convolution or flatten pixel values. However, for this example, we will use another approach used in the paper, where the authors flatten the pixels by applying a 1D convolution and add learned absolute positional 1D position embeddings. Based on the paper, the other approaches provide better results, but what’s unique with this pre-processing approach is that it doesn’t provide any information about the 2D image structure.\nLet’s assume we have a tensor representing a batch of images of shape (batch_size, 3, 32, 32). So each image has a width and height of 32 and 3 channels. To pre-process this image we will:\n\nApply a Conv1D layer to increase the number of channels, for example to 256. So now, let’s say our output has a shape of (batch_size, 256, 32, 32).\nTransform the tensor returned by Conv1D from channel first to channel last. Our new shape is (batch_size, 32, 32, 256).\nFlatten the height with the width of the image. So now we have a shape of (batch_size, 1024, 256).\nInstantiate a trainable 1D position embedding for each pixel with, for example, 256 channels. So the shape of the embedding is (batch_size, 1024, 256).\nConcatenate the output of the Conv1D layer with the trainable 1D position embedding based on the last dimension. So the final shape will be (batch_size, 1024, 512).\n\nThat’s it! Let’s implement it.\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, index_dim, num_channels=128):\n        super().__init__()\n        self.index_dim = index_dim\n        self.num_channels = num_channels\n        self.position_embeddings = nn.Parameter(torch.randn(index_dim, num_channels))\n\n    def forward(self, batch_size):\n        return self.position_embeddings.expand(batch_size, -1, -1)\n\n\nclass ImagePreProcessor(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        spatial_downsample,\n        position_encoding_index_dim,\n        position_encoding_out_channels,\n    ):\n        super().__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.spatial_downsample = spatial_downsample\n        self.postion_encoding_index_dim = position_encoding_index_dim\n\n        self.conv1d = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(1, 1),\n            stride=(spatial_downsample, spatial_downsample),\n        )\n\n        self.pos_enc = PositionalEncoding(\n            position_encoding_index_dim, num_channels=position_encoding_out_channels\n        )\n\n    def forward(self, inputs):\n        batch_size = inputs.shape[0]\n        # Increase the number of channels while keeping same height and width\n        inputs_post_conv1d = self.conv1d(inputs)\n        # Make channel last\n        inputs = torch.moveaxis(inputs, 1, -1)\n        # Flatten from (batch_size, img_size, img_size, num_channels) to (batch_size, img_size*img_size, num_channels)\n        inputs_post_conv1d = torch.reshape(\n            inputs_post_conv1d, [batch_size, np.prod(inputs.shape[1:-1]), -1]\n        )\n        # Instantiate learned 1D positional embeddings\n        pos_encoded = self.pos_enc(batch_size)\n        # Concat inputs post conv1d with 1D positional embeddings\n        return torch.cat([inputs_post_conv1d, pos_encoded], dim=-1)\n\nWe can validate we get the expected output shape using an image of shape (3, 32, 32) with Conv1D output channels of 256 and a 1D positional embeddings with 256 channels.\n\nx = torch.ones((1, 3, 32, 32))\nimage_processor = ImagePreProcessor(3, 256, 1, 32**2, 256)\nprocessed_image = image_processor(x)\n\nprint(f\"Image shape: {x.shape}\")\nprint(f\"Processed image shape: {processed_image.shape}\")\n\nImage shape: torch.Size([1, 3, 32, 32])\nProcessed image shape: torch.Size([1, 1024, 512])\n\n\nExcellent, as expected our tensor has a shape of (1, 1024, 512). That’s all we need to pre-process our images.\nWe can finally train an image classification model using the Perceiver architecture!\n\n\nTraining\nTo demonstrate we can use the Perceiver IO architecture to classify images, we will use the MNIST dataset. Here, we are using a simple dataset to validate our model can classify images. However, note that in the paper, the authors were able to achieve strong results on the ImageNet dataset, especially when the model was pre-trained and the pre-processor used 2D convolution and MaxPooling layers.\n\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import datasets\nfrom torchvision import transforms\n\nfrom sklearn.metrics import accuracy_score\n\ndevice = \"cuda\" if torch.has_cuda else \"cpu\"\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n)\n\ntraining_data = datasets.MNIST(\n    root=\"data\", train=True, download=True, transform=transform\n)\n\neval_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n\n\ntrain_dataloader = DataLoader(training_data, batch_size=128, shuffle=True)\neval_dataloader = DataLoader(eval_data, batch_size=128, shuffle=True)\n\n\nimg_size = 28\nimg_channels = 1\nimg_processor_output_channels = 32\nimg_processor_pos_encoding_out_channels = 32\nn_labels = 10\ninput_dim = 64\nlatent_embedding_dim = 128\nnum_latents = 258\nquery_dim = 128\nqk_channels = query_dim\nv_channels = qk_channels\nnum_heads = 1\nnb_processors = 3\nwidening_factor = 1\nspatial_downsample = 1\n\n\nimage_processor = ImagePreProcessor(\n    img_channels,\n    img_processor_output_channels,\n    spatial_downsample,\n    img_size**2,\n    img_processor_pos_encoding_out_channels,\n)\n\nmodel = PerceiverIO(\n    n_labels,\n    input_dim,\n    latent_embedding_dim,\n    query_dim,\n    qk_channels,\n    v_channels,\n    num_latents,\n    num_heads,\n    nb_processors,\n    widening_factor,\n    input_processor=image_processor,\n)\n\nmodel = model.to(device)\n\nLet’s define a generic training and evaluation loop we can reuse later to classify text.\n\ndef train(model, loss_fn, device, train_loader, optimizer, epoch, log_interval=50):\n    model.train()\n    for batch_idx, (x, y) in enumerate(train_loader):\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = loss_fn(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        pred = logits.argmax(-1).cpu().numpy()\n        acc = accuracy_score(y_true=y.cpu().numpy(), y_pred=pred)\n\n        if batch_idx % log_interval == 0:\n            print(\n                f\"Train Epoch: {epoch} [{batch_idx * len(x)}/{len(train_loader.dataset)} ({(100. * batch_idx  * len(x) / len(train_loader.dataset)):.2f}%)]\\t\"\n                f\"Loss: {loss.item():.2f} - Accuracy: {acc:.2f}\"\n            )\n\n\ndef eval(model, loss_fn, device, eval_loader):\n    model.eval()\n    eval_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for x, y in eval_loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            eval_loss += loss_fn(logits, y).item()\n            pred = logits.argmax(-1, keepdim=True)\n            correct += pred.eq(y.view_as(pred)).sum().item()\n\n    eval_loss /= len(eval_loader.dataset)\n\n    print(\n        f\"\\nTest set: Average loss: {eval_loss:.2f}, Accuracy: \"\n        f\"{(100. * correct/len(eval_loader.dataset)):.2f}%\\n\"\n    )\n\nTo train this model we will use the Adam optimizer with weight decay from Pytorch because we’re working with a simple dataset. However, in practice, it’s recommended to use an Adam algorithm with a fix weight decay as introduced in Decoupled Weight Decay Regularization (Loshchilov et al, 2019). You can find an implementation in the transformer library. It’s also recommended to use a scheduler with a warm-up phase. If you want to learn more about the importance of using adaptive optimizers and learning rate warm-up when training transformers from scratch, I highly recommend this blog post from Borealise AI.\n\nEPOCHS = 12\nloss_fn = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=4e-3, weight_decay=1e-1)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n\n\nfor e in range(EPOCHS):\n    train(model, loss_fn, device, train_dataloader, optimizer, e, log_interval=100)\n    eval(model, loss_fn, device, eval_dataloader)\n    scheduler.step()\n\nTrain Epoch: 0 [0/60000 (0.00%)]    Loss: 2.59 - Accuracy: 0.12\nTrain Epoch: 0 [12800/60000 (21.33%)]   Loss: 1.08 - Accuracy: 0.64\nTrain Epoch: 0 [25600/60000 (42.67%)]   Loss: 0.64 - Accuracy: 0.79\nTrain Epoch: 0 [38400/60000 (64.00%)]   Loss: 0.57 - Accuracy: 0.85\nTrain Epoch: 0 [51200/60000 (85.33%)]   Loss: 0.60 - Accuracy: 0.79\n\nTest set: Average loss: 0.00, Accuracy: 87.84%\n\nTrain Epoch: 1 [0/60000 (0.00%)]    Loss: 0.43 - Accuracy: 0.85\nTrain Epoch: 1 [12800/60000 (21.33%)]   Loss: 0.26 - Accuracy: 0.91\nTrain Epoch: 1 [25600/60000 (42.67%)]   Loss: 0.36 - Accuracy: 0.90\nTrain Epoch: 1 [38400/60000 (64.00%)]   Loss: 0.27 - Accuracy: 0.91\nTrain Epoch: 1 [51200/60000 (85.33%)]   Loss: 0.35 - Accuracy: 0.90\n\nTest set: Average loss: 0.00, Accuracy: 93.31%\n\nTrain Epoch: 2 [0/60000 (0.00%)]    Loss: 0.19 - Accuracy: 0.95\nTrain Epoch: 2 [12800/60000 (21.33%)]   Loss: 0.16 - Accuracy: 0.95\nTrain Epoch: 2 [25600/60000 (42.67%)]   Loss: 0.14 - Accuracy: 0.95\nTrain Epoch: 2 [38400/60000 (64.00%)]   Loss: 0.30 - Accuracy: 0.94\nTrain Epoch: 2 [51200/60000 (85.33%)]   Loss: 0.26 - Accuracy: 0.91\n\nTest set: Average loss: 0.00, Accuracy: 94.85%\n\nTrain Epoch: 3 [0/60000 (0.00%)]    Loss: 0.13 - Accuracy: 0.95\nTrain Epoch: 3 [12800/60000 (21.33%)]   Loss: 0.10 - Accuracy: 0.98\nTrain Epoch: 3 [25600/60000 (42.67%)]   Loss: 0.23 - Accuracy: 0.93\nTrain Epoch: 3 [38400/60000 (64.00%)]   Loss: 0.16 - Accuracy: 0.95\nTrain Epoch: 3 [51200/60000 (85.33%)]   Loss: 0.10 - Accuracy: 0.98\n\nTest set: Average loss: 0.00, Accuracy: 95.12%\n\nTrain Epoch: 4 [0/60000 (0.00%)]    Loss: 0.21 - Accuracy: 0.95\nTrain Epoch: 4 [12800/60000 (21.33%)]   Loss: 0.10 - Accuracy: 0.96\nTrain Epoch: 4 [25600/60000 (42.67%)]   Loss: 0.19 - Accuracy: 0.95\nTrain Epoch: 4 [38400/60000 (64.00%)]   Loss: 0.18 - Accuracy: 0.94\nTrain Epoch: 4 [51200/60000 (85.33%)]   Loss: 0.10 - Accuracy: 0.97\n\nTest set: Average loss: 0.00, Accuracy: 95.99%\n\nTrain Epoch: 5 [0/60000 (0.00%)]    Loss: 0.06 - Accuracy: 0.97\nTrain Epoch: 5 [12800/60000 (21.33%)]   Loss: 0.05 - Accuracy: 0.99\nTrain Epoch: 5 [25600/60000 (42.67%)]   Loss: 0.12 - Accuracy: 0.98\nTrain Epoch: 5 [38400/60000 (64.00%)]   Loss: 0.11 - Accuracy: 0.98\nTrain Epoch: 5 [51200/60000 (85.33%)]   Loss: 0.16 - Accuracy: 0.97\n\nTest set: Average loss: 0.00, Accuracy: 96.51%\n\nTrain Epoch: 6 [0/60000 (0.00%)]    Loss: 0.14 - Accuracy: 0.97\nTrain Epoch: 6 [12800/60000 (21.33%)]   Loss: 0.15 - Accuracy: 0.95\nTrain Epoch: 6 [25600/60000 (42.67%)]   Loss: 0.04 - Accuracy: 0.99\nTrain Epoch: 6 [38400/60000 (64.00%)]   Loss: 0.04 - Accuracy: 1.00\nTrain Epoch: 6 [51200/60000 (85.33%)]   Loss: 0.08 - Accuracy: 0.98\n\nTest set: Average loss: 0.00, Accuracy: 96.94%\n\nTrain Epoch: 7 [0/60000 (0.00%)]    Loss: 0.04 - Accuracy: 0.98\nTrain Epoch: 7 [12800/60000 (21.33%)]   Loss: 0.17 - Accuracy: 0.95\nTrain Epoch: 7 [25600/60000 (42.67%)]   Loss: 0.01 - Accuracy: 1.00\nTrain Epoch: 7 [38400/60000 (64.00%)]   Loss: 0.09 - Accuracy: 0.96\nTrain Epoch: 7 [51200/60000 (85.33%)]   Loss: 0.18 - Accuracy: 0.96\n\nTest set: Average loss: 0.00, Accuracy: 97.11%\n\nTrain Epoch: 8 [0/60000 (0.00%)]    Loss: 0.09 - Accuracy: 0.97\nTrain Epoch: 8 [12800/60000 (21.33%)]   Loss: 0.13 - Accuracy: 0.94\nTrain Epoch: 8 [25600/60000 (42.67%)]   Loss: 0.09 - Accuracy: 0.98\nTrain Epoch: 8 [38400/60000 (64.00%)]   Loss: 0.12 - Accuracy: 0.95\nTrain Epoch: 8 [51200/60000 (85.33%)]   Loss: 0.14 - Accuracy: 0.95\n\nTest set: Average loss: 0.00, Accuracy: 96.95%\n\nTrain Epoch: 9 [0/60000 (0.00%)]    Loss: 0.07 - Accuracy: 0.98\nTrain Epoch: 9 [12800/60000 (21.33%)]   Loss: 0.07 - Accuracy: 0.98\nTrain Epoch: 9 [25600/60000 (42.67%)]   Loss: 0.05 - Accuracy: 0.98\nTrain Epoch: 9 [38400/60000 (64.00%)]   Loss: 0.07 - Accuracy: 0.98\nTrain Epoch: 9 [51200/60000 (85.33%)]   Loss: 0.07 - Accuracy: 0.98\n\nTest set: Average loss: 0.00, Accuracy: 97.00%\n\nTrain Epoch: 10 [0/60000 (0.00%)]   Loss: 0.04 - Accuracy: 0.98\nTrain Epoch: 10 [12800/60000 (21.33%)]  Loss: 0.08 - Accuracy: 0.95\nTrain Epoch: 10 [25600/60000 (42.67%)]  Loss: 0.10 - Accuracy: 0.98\nTrain Epoch: 10 [38400/60000 (64.00%)]  Loss: 0.08 - Accuracy: 0.98\nTrain Epoch: 10 [51200/60000 (85.33%)]  Loss: 0.10 - Accuracy: 0.97\n\nTest set: Average loss: 0.00, Accuracy: 97.32%\n\nTrain Epoch: 11 [0/60000 (0.00%)]   Loss: 0.09 - Accuracy: 0.96\nTrain Epoch: 11 [12800/60000 (21.33%)]  Loss: 0.04 - Accuracy: 0.99\nTrain Epoch: 11 [25600/60000 (42.67%)]  Loss: 0.14 - Accuracy: 0.96\nTrain Epoch: 11 [38400/60000 (64.00%)]  Loss: 0.14 - Accuracy: 0.97\nTrain Epoch: 11 [51200/60000 (85.33%)]  Loss: 0.04 - Accuracy: 0.98\n\nTest set: Average loss: 0.00, Accuracy: 97.43%\n\n\n\nAfter training our model we achieved ~97% accuracy. If we were spending more time on tuning the model, using a 2D Fourier or 2D convolutional pre-processor, it’s very likely we could achieve a higher accuracy. Nonetheless, we were able to classify images with the Perceiver model with zero information about the 2D structure of the images.\nLet’s now explore how we can use the same architecture for a text classification task."
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#text-classification",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#text-classification",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Text Classification",
    "text": "Text Classification\n\nPre-Processing\nFor this example, we will use the AG_NEWS dataset which contains a corpus of news articles. Each article is classified as one of the following categories: World, Sports, Business or Sci/Tech.\n\nfrom torchtext.datasets import AG_NEWS\nfrom torchtext.data.functional import to_map_style_dataset\n\ntrain_iter = AG_NEWS(split=\"train\")\neval_iter = AG_NEWS(split=\"test\")\n\ntrain_dataset = to_map_style_dataset(train_iter)\neval_dataset = to_map_style_dataset(eval_iter)\n\nWith Perceiver IO, tokenization is extremely simple: you just need to convert the string to raw UTF-8 bytes. You don’t need to apply more sophisticated tokenizers such as WordPiece or SentencePiece or BPE, etc. Then for the numericalization process, we will just convert each byte to a byte ID. Finally, we will pad the sequence to a max sequence length. The main reasons why Perceiver IO aims to get rid of tokenizers is because they tend to perform less well on rare words, and they don’t transfer well from one language to another.\nIn the paper implementation, the authors have some reserved tokens such as [BOS], [EOS], [SEP], etc., to represent the beginning of the sentence and the end of the sentence. For simplicity, we will just convert the sentence to raw bytes, then ID.\n\nclass BytesTokenizer:\n    def __init__(self):\n        pass\n\n    def to_int(self, inputs):\n        if isinstance(inputs, str):\n            inputs = inputs.encode(\"utf-8\")\n        encoded = torch.frombuffer(inputs, dtype=torch.uint8).to(torch.int32)\n        return encoded.to(torch.int32)\n\n    @property\n    def vocab_size(self):\n        return 256\n\nWe can validate the tokenizer is working properly with an example.\n\ninput = \"Hello Hello\"\ntokenizer = BytesTokenizer()\ntokenized_input = tokenizer.to_int(input)\nprint(f\"Sentence: {input}\")\nprint(f\"Tokenized sentence: {tokenized_input}\")\n\nSentence: Hello Hello\nTokenized sentence: tensor([ 72, 101, 108, 108, 111,  32,  72, 101, 108, 108, 111],\n       dtype=torch.int32)\n\n\n/tmp/ipykernel_420991/1393707225.py:8: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:1111.)\n  encoded = torch.frombuffer(inputs, dtype=torch.uint8).to(torch.int32)\n\n\nWe can see the number of tokens matches the number of characters in the original sentence. Additionally, the word “Hello” repeats twice.\nUnfortunately, when training the model with this tokenizer to classify the articles, I discovered the model was converging very slowly and didn’t give strong results. Instead, we will use a simple word tokenizer. I think one of the reasons I was getting poor results on this task with the byte tokenizer was because the presence of certain words is highly correlated with the topics in the news articles. Here, we are training the model from scratch, so the model starts without a language representation. Therefore it’s harder for the model to identify that certain sequences of bytes represents certain words which are relevant to the classification tasks. Usually these models get pre-trained on a masked language modeling task (fill in the missing/masked word in the sentence) to build a language representation, then gets fine-tuned on a text classification task. During the first step the model is able to build a language representation using the bytes tokenizer. As demonstrated in the paper, the authors get very strong results on the Glue benchmark. I tried to fine-tune a Perceiver IO model from the transformer library using a byte tokenizer on this task, and I also got good results. Therefore, the challenges probably appear when training the model from scratch with byte tokenizers on certain tasks such as text classification.\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer(\"basic_english\")\n\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n\nvocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\nNext step is to implement a padding function so all the sequences in the batch will have the same length. If the length of the sequence exceeds the maximum sequence length, it will be truncated.\n\ndef pad(max_sequence_length, inputs, pad_value=0):\n    input_len = inputs.shape[1]\n    # Truncate sequence if exceeds max sequence length\n    if input_len > max_sequence_length:\n        inputs = inputs[:max_sequence_length]\n    # Pad sequence with pad value if shorter the max sequence length\n    pad_len = max_sequence_length - input_len\n    padded_input = torch.nn.functional.pad(inputs, pad=((0, pad_len)), value=pad_value)\n    return padded_input\n\nNow that we have our tokenizer and padding function, we can create a collate_batch function. This function will be used by our DataLoader to tokenize and pad each sentence contained in the batch.\n\nMAX_SEQUENCE_LEN = 256\n\n\ndef collate_batch(batch):\n    label_list, text_list = [], []\n\n    for (_label, _text) in batch:\n        # Convert labels [1, 2, 3, 4] to [0, 1, 2, 3] for loss function\n        label_processed = _label - 1\n        label_list.append(label_processed)\n        # Tokenize and numericalize sentence\n        tokenized_text = torch.unsqueeze(\n            torch.tensor(vocab(tokenizer(_text)), dtype=torch.int64), 0\n        )\n        # Pad and truncate the tokenized sentence to match MAX_SEQUENCE_LEN\n        padded_text = pad(MAX_SEQUENCE_LEN, tokenized_text)\n        text_list.append(padded_text)\n\n    label_list = torch.tensor(label_list, dtype=torch.int64)\n    text_list = torch.cat(text_list, dim=0)\n    return text_list.to(device), label_list.to(device)\n\nWe are just missing one piece of the puzzle before starting to train our model: a text processing layer. This processing layer is very similar to the one you could find in a Bert model. It consists of:\n\nConverting each token in the sentence to embeddings\nRepresenting the position of each token as embeddings\nAdding the token embeddings to the function embeddings\n\n\nclass TextProcessor(nn.Module):\n    def __init__(self, vocab_size, d_model, max_position_embeddings=256):\n        super().__init__()\n        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n\n    def forward(self, inputs):\n        embeddings = self.embeddings(inputs)\n        seq_len = inputs.shape[1]\n        position_ids = torch.arange(0, seq_len, device=inputs.device)\n        embeddings = embeddings + self.position_embeddings(position_ids)\n        return embeddings\n\n\n\nTraining\nWe can finally train our model to classify news articles. First we create a Dataloader for the training and evaluation datasets using the collate_batch function previously defined.\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=128, shuffle=True, collate_fn=collate_batch\n)\neval_dataloader = DataLoader(\n    eval_dataset, batch_size=128, shuffle=True, collate_fn=collate_batch\n)\n\nThen we instantiate the Perceiver model. This is the exact same model we used for the image classification task, except we use a text processor layer, set the number of labels to 4, and adjust the input_dim parameter to match the embedding length for each token.\n\nvocab_size = len(vocab.vocab)\nn_labels = 4\ninput_dim = MAX_SEQUENCE_LEN\nlatent_embedding_dim = 64\nnum_latents = 64\nquery_dim = 64\nqk_channels = query_dim\nv_channels = qk_channels\nnum_heads = 4\nnb_processor = 2\nwidening_factor = 1\n\ntext_processor = TextProcessor(vocab_size, input_dim)\n\nmodel = PerceiverIO(\n    n_labels,\n    input_dim,\n    latent_embedding_dim,\n    query_dim,\n    qk_channels,\n    v_channels,\n    num_latents,\n    num_heads,\n    nb_processor,\n    widening_factor,\n    input_processor=text_processor,\n)\n\nmodel = model.to(device)\n\nTo train the model we can reuse the training and evaluation loop from the previous tasks.\n\nEPOCHS = 2\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = AdamW(model.parameters(), lr=2e-3, weight_decay=1e-1)\n\nfor e in range(EPOCHS):\n    train(model, loss_fn, device, train_dataloader, optimizer, e, log_interval=250)\n    eval(model, loss_fn, device, eval_dataloader)\n\nTrain Epoch: 0 [0/120000 (0.00%)]   Loss: 1.58 - Accuracy: 0.23\nTrain Epoch: 0 [32000/120000 (26.67%)]  Loss: 0.38 - Accuracy: 0.89\nTrain Epoch: 0 [64000/120000 (53.33%)]  Loss: 0.33 - Accuracy: 0.91\nTrain Epoch: 0 [96000/120000 (80.00%)]  Loss: 0.31 - Accuracy: 0.89\n\nTest set: Average loss: 0.00, Accuracy: 91.30%\n\nTrain Epoch: 1 [0/120000 (0.00%)]   Loss: 0.19 - Accuracy: 0.95\nTrain Epoch: 1 [32000/120000 (26.67%)]  Loss: 0.24 - Accuracy: 0.91\nTrain Epoch: 1 [64000/120000 (53.33%)]  Loss: 0.17 - Accuracy: 0.95\nTrain Epoch: 1 [96000/120000 (80.00%)]  Loss: 0.24 - Accuracy: 0.92\n\nTest set: Average loss: 0.00, Accuracy: 91.70%\n\n\n\n~91% accuracy on the evaluation set, not bad. With the same architecture we were able to tackle two distinct tasks."
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#conclusion",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#conclusion",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nBy showing you how we can implement this architecture from scratch, I hope I’ve given you a better idea about how the Perceiver architecture can solve a wide variety of tasks by just modifying the pre-processing and post-processing steps while keeping the rest of the architecture tasks agnostic. In this post, we focused our efforts on image classification and text classification tasks, but there’s still so much more you can do. In the transformers library, as well as Niels Rogge’s blog post, you can learn how to implement other pre-processor and post-processor modules to tackle a wide variety of tasks, such as masked language model, optical flow, and more. We could easily extend these modules to question answering or object detection as well. Where this architecture really shines is with multi-modal tasks. One example is this multi-modal auto-encoding task where the authors were able to accurately reconstruct multi-modal inputs consisting of a sequence of images, audio recordings, and class labels. In the perceiver IO paper, the authors were even able to replace the original transformer model in AlphaStar (Arulkumaran et al, 2019) and match state-of-the-art results. Honestly, the possibilities are endless, so go have fun :)."
  },
  {
    "objectID": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#ressources",
    "href": "posts/perceiver-io-experiment/perceiver_io_from_scratch.html#ressources",
    "title": "A General Purpose Deep Learning Architecture - Perceiver IO From Scratch",
    "section": "Ressources",
    "text": "Ressources\n\nPerceiver IO: A General Architecture for Structured Inputs & Outputs (Jaegle et al, 2021)\nCS25 I Stanford Seminar - DeepMind’s Perceiver and Perceiver IO: new data family architecture\nPerceiver IO: a scalable, fully-attentional model that works on any modality\nPerceiver IO implementation from DeepMind\nPerceiver IO implementation from HuggingFace Transformers\nNatural Language Processing with Transformers book"
  },
  {
    "objectID": "posts/moose-hf-inference/moose_hf_blog.html",
    "href": "posts/moose-hf-inference/moose_hf_blog.html",
    "title": "Encrypted Inference with Moose and HuggingFace",
    "section": "",
    "text": "The other day, I was very inspired by the blog post Sentiment Analysis on Encrypted Data with Homomorphic Encryption co-written by Zama and HuggingFace. Zama has created an excellent encrypted machine learning library, Concrete-ML, based on fully homomorphic encryption (FHE). Concrete-ML enables data scientists to easily turn their machine learning models into an homomorphic equivalent in order to perform inference on encrypted data. In the blog post, the authors demonstrate how you can easily perform sentiment analysis on encrypted data with this library. As you can imagine, sometimes you will need to perform sentiment analysis on text containing sensitive information. With FHE, the data always remains encrypted during computation, which enables data scientists to provide a machine learning service to a user while maintaining data confidentiality.\nThe last several years, I was very fortunate to also work at the intersection of machine learning and cryptography. One of my collaborations with Morten Dahl, Jason Mancuso, Dragos Roturu and Lex Verona that I am very excited about is Moose. Moose is a distributed dataflow framework for encrypted machine learning and data processing. Moose’s cryptographic protocol is based on secure multi-party-computation (MPC). Depending on the scenario, FHE and MPC have different pros and cons. Currently MPC generally tends to be more performant, however the protocol requires 2 or 3 non-colluding parties (e.g a data owner and a data scientist) willing to perform computations together. If you want to learn about MPC in the context of machine learning, I highly recommend this very comprehensive blog post where Morten implements an MPC protocol from scratch for Deep Learning.\nIn the rest of this blog post, I will show how you can perform encrypted inference with Moose using the sentiment analysis use case from Zama and HuggingFace’s blog post."
  },
  {
    "objectID": "posts/moose-hf-inference/moose_hf_blog.html#model-training",
    "href": "posts/moose-hf-inference/moose_hf_blog.html#model-training",
    "title": "Encrypted Inference with Moose and HuggingFace",
    "section": "Model Training",
    "text": "Model Training\nThe sentiment analysis model will be trained on the Twitter US Airline Sentiment dataset from Kaggle. To train the model, we will use the code provided in the blog post. The sentiment model consists of a RoBERTa (Liu et al, 2019) transformer to extract features from the text, and an XGBoost model on top of it to classify the tweets into positive, negative, or neutral classes.\n\nimport os\nimport tqdm\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom xgboost.sklearn import XGBClassifier\n\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\n\nLet’s first load the dataset.\n\nif not os.path.isfile(\"local_datasets/twitter-airline-sentiment/Tweets.csv\"):\n    raise ValueError(\"Please launch the `download_data.sh` script to get datasets\")\n\ntrain = pd.read_csv(\"local_datasets/twitter-airline-sentiment/Tweets.csv\", index_col=0)\ntext_X, y = train[\"text\"], train[\"airline_sentiment\"] \ny = y.replace([\"negative\", \"neutral\", \"positive\"], [0, 1, 2])\n\npos_ratio = y.value_counts()[2] / y.value_counts().sum()\nneg_ratio = y.value_counts()[0] / y.value_counts().sum()\nneutral_ratio = y.value_counts()[1] / y.value_counts().sum()\n\nprint(f\"Proportion of positive examples: {round(pos_ratio * 100, 2)}%\")\nprint(f\"Proportion of negative examples: {round(neg_ratio * 100, 2)}%\")\nprint(f\"Proportion of neutral examples: {round(neutral_ratio * 100, 2)}%\")\n\nProportion of positive examples: 16.14%\nProportion of negative examples: 62.69%\nProportion of neutral examples: 21.17%\n\n\nAs you can see the tweets are classified into three categories: positive, negative and neutral.\nFor the feature extractor, in the blog post, the authors use a RoBerta transformer pre-trained on Tweets.\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the tokenizer (converts text to tokens)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n)\n\n# Load the pre-trained model\ntransformer_model = AutoModelForSequenceClassification.from_pretrained(\n    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n)\n\nSome weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\nThe function below will be responsible for extracting the features from the tweets.\n\n# Function that transforms a list of texts to their representation\n# learned by the transformer.\ndef text_to_tensor(\n    list_text_X_train: list,\n    transformer_model: AutoModelForSequenceClassification,\n    tokenizer: AutoTokenizer,\n    device: str,\n) -> np.ndarray:\n    # Tokenize each text in the list one by one\n    tokenized_text_X_train_split = []\n    for text_x_train in list_text_X_train:\n        tokenized_text_X_train_split.append(\n            tokenizer.encode(text_x_train, return_tensors=\"pt\")\n        )\n\n    # Send the model to the device\n    transformer_model = transformer_model.to(device)\n    output_hidden_states_list = []\n\n    for tokenized_x in tqdm.tqdm(tokenized_text_X_train_split):\n        # Pass the tokens through the transformer model and get the hidden states\n        # Only keep the last hidden layer state for now\n        output_hidden_states = transformer_model(\n            tokenized_x.to(device), output_hidden_states=True\n        )[1][-1]\n        # Average over the tokens axis to get a representation at the text level.\n        output_hidden_states = output_hidden_states.mean(dim=1)\n        output_hidden_states = output_hidden_states.detach().cpu().numpy()\n        output_hidden_states_list.append(output_hidden_states)\n\n    return np.concatenate(output_hidden_states_list, axis=0)\n\nWe are now ready to run the feature extractor on the training and testing set, then train the XGBoost model on the feature extractor’s output.\n\n# Split in train test\ntext_X_train, text_X_test, y_train, y_test = train_test_split(\n    text_X, y, test_size=0.1, random_state=42\n)\n\n# Let's vectorize the text using the transformer\nlist_text_X_train = text_X_train.tolist()\nlist_text_X_test = text_X_test.tolist()\n\nX_train_transformer = text_to_tensor(\n    list_text_X_train, transformer_model, tokenizer, device\n)\nX_test_transformer = text_to_tensor(\n    list_text_X_test, transformer_model, tokenizer, device\n)\n\n# Let's build our model\nmodel = XGBClassifier()\n\n# A gridsearch to find the best parameters\nparameters = {\n    \"max_depth\": [1],\n    \"n_estimators\": [10, 30, 50],\n    \"n_jobs\": [-1],\n}\n\n# Now we have a representation for each tweet, we can train a model on these.\ngrid_search = GridSearchCV(model, parameters, cv=3, n_jobs=1, scoring=\"accuracy\")\ngrid_search.fit(X_train_transformer, y_train)\n\n# Check the accuracy of the best model\nprint(f\"Best score: {grid_search.best_score_}\")\n\n# Check best hyperparameters\nprint(f\"Best parameters: {grid_search.best_params_}\")\n\n# Extract best model\nbest_model = grid_search.best_estimator_\n\n# Compute the metrics for each class\n\ny_proba = best_model.predict_proba(X_test_transformer)\n\n# Compute the accuracy\ny_pred = np.argmax(y_proba, axis=1)\naccuracy_transformer_xgboost = np.mean(y_pred == y_test)\nprint(f\"Accuracy: {accuracy_transformer_xgboost:.4f}\")\n\ny_pred_positive = y_proba[:, 2]\ny_pred_negative = y_proba[:, 0]\ny_pred_neutral = y_proba[:, 1]\n\nap_positive_transformer_xgboost = average_precision_score(\n    (y_test == 2), y_pred_positive\n)\nap_negative_transformer_xgboost = average_precision_score(\n    (y_test == 0), y_pred_negative\n)\nap_neutral_transformer_xgboost = average_precision_score((y_test == 1), y_pred_neutral)\n\nprint(\n    f\"Average precision score for positive class: \"\n    f\"{ap_positive_transformer_xgboost:.4f}\"\n)\nprint(\n    f\"Average precision score for negative class: \"\n    f\"{ap_negative_transformer_xgboost:.4f}\"\n)\nprint(\n    f\"Average precision score for neutral class: \"\n    f\"{ap_neutral_transformer_xgboost:.4f}\"\n)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n100%|██████████| 13176/13176 [11:32<00:00, 19.02it/s]\n100%|██████████| 1464/1464 [01:17<00:00, 18.78it/s]\n\n\nBest score: 0.844869459623558\nBest parameters: {'max_depth': 1, 'n_estimators': 50, 'n_jobs': -1}\nAccuracy: 0.8559\nAverage precision score for positive class: 0.9015\nAverage precision score for negative class: 0.9675\nAverage precision score for neutral class: 0.7517\n\n\nExcellent, we have a sentiment analysis model with an 85% accuracy. We can run the model on a sample tweet.\n\ntested_tweet = [\"AirFrance is awesome, almost as much as Zama!\"]\nX_tested_tweet = text_to_tensor(tested_tweet, transformer_model, tokenizer, device)\nnp.save(\"data/x_tested_tweet.npy\", X_tested_tweet)\nclear_proba = best_model.predict_proba(X_tested_tweet)\nprint(f\"Proba prediction in plaintext {clear_output}\")\n\n100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n\n\nClear_proba [[0.02582786 0.02599407 0.94817805]]"
  },
  {
    "objectID": "posts/moose-hf-inference/moose_hf_blog.html#encrypted-inference-with-moose",
    "href": "posts/moose-hf-inference/moose_hf_blog.html#encrypted-inference-with-moose",
    "title": "Encrypted Inference with Moose and HuggingFace",
    "section": "Encrypted Inference with Moose",
    "text": "Encrypted Inference with Moose\nNow that we have a model trained, we are ready to serve encrypted inference with Moose. For simplicity, we will start by locally prototyping this computation happening between the different parties using the pm.LocalMooseRuntime.\nTo serve encrypted inference, we will have to perform the following steps: - Convert the trained model to ONNX format. - Convert the model from ONNX to a Moose computation. - Run encrypted inference by evaluating the Moose computation.\nLet’s get started!\n\nfrom onnxmltools.convert import convert_xgboost\nfrom skl2onnx.common import data_types as onnx_dtypes\n\nimport pymoose as pm\n\n\nConvert to ONNX\nWe can convert the XGBoost model into an ONNX proto using the convert_xgboos method from the onnxmltools.\n\nn_features = X_test_transformer[0].shape[0]\ninitial_type = (\"float_input\", onnx_dtypes.FloatTensorType([None, n_features]))\nonnx_proto = convert_xgboost(best_model, initial_types=[initial_type])\n\n\n\nConvert ONNX to Moose Predictor\nPyMoose provides several predictor classes to translate an ONNX model into a PyMoose DSL program. Because the trained model is an XGBoost model, we can use the class tree_ensemble.TreeEnsembleClassifier. The class has a method from_onnx which will parse the ONNX file. The returned object is callable. When called, it will compute the forward pass of the XGBoost model.\n\npredictor = pm.predictors.TreeEnsembleClassifier.from_onnx(onnx_proto)\n\n\n\nDefine Moose Computation\nTo express this computation, Moose offers a Python DSL (internally referred to as the eDSL, i.e. “embedded” DSL). As you will notice, the syntax is very similar to the scientific computation library Numpy.\nThe main difference is the notion of placements. There are two types of placements: host placement and replicated placement. With Moose, every operation under a host placement context is computed on plaintext values (not encrypted). Every operation under a replicated placement is performed on secret shared values (encrypted).\nWe will compute the inference between three different players, each of them representing a host placement: a data owner, a data scientist, and a third party. The three players are grouped under the replicated placement to perform the encrypted computation. Currently, the MPC protocol of Moose expects three parties, but other MPC schemes can expect two parties. practice, the third party could be a secure enclave that the data scientist and data owner can’t access.\nWhen we have instantiated the pm.predictors.TreeEnsembleClassifier class, under the hood three host placements have been instiated: alice, bob and carole. For our use case, alice will represent the data owner, bob the model owner and carole the third party.\nThe Moose computation below performs the following steps:\n\nLoads the tweet (after running the feature extractor) in plaintext from alice’s (data owner) storage.\nSecret share (encrypts) the data.\nComputes XGBoost inference on secret shared data.\nReveals the prediction only to alice (the data owner) and saves it into its storage.\n\n\n@pm.computation\ndef moose_predictor_computation():\n    # Alice (data owner) load their data in plaintext\n    # Then the data gets converted from float to fixed-point\n    with predictor.alice:\n        x = pm.load(\"x\", dtype=pm.float64)\n        x_fixed = pm.cast(x, dtype=pm.predictors.predictor_utils.DEFAULT_FIXED_DTYPE)\n    # The data gets secret shared when moving from host placement\n    # to replicated placement.\n    # Then compute the logistic regression on secret shared data\n    with predictor.replicated:\n        y_pred = predictor(x_fixed, pm.predictors.predictor_utils.DEFAULT_FIXED_DTYPE)\n\n    # The predictions gets revealed only to Alice (the data owner)\n    # Convert the data from fixed-point to floats and save the data in the storage\n    with predictor.alice:\n        y_pred = pm.cast(y_pred, dtype=pm.float64)\n        y_pred = pm.save(\"y_pred\", y_pred)\n    return y_pred\n\n\n\nEvaluate the computation\nFor simplicity, we will use pm.LocalMooseRuntime to locally simulate this computation running across hosts. To do so, we need to provide: the Moose computation, the list of host identities to simulate, and a mapping of the data stored by each simulated host.\nSince the data owner is represented by alice, we will place the patients’ data in alice’s storage.\nOnce you have instantiated the pm.LocalMooseRuntime with the identities and additional storage mapping and the runtime set as default, you can simply call the Moose computation to evaluate it.\n\nexecutive_storage = {\n    \"alice\": {\"x\": X_tested_tweet.astype(np.float64)},\n    \"bob\": {},\n    \"carole\": {},\n}\nidentities = [plc.name for plc in predictor.host_placements]\n\nruntime = pm.LocalMooseRuntime(identities, storage_mapping=executive_storage)\nruntime.set_default()\n\n_ = moose_predictor_computation()\n\nOnce the computation is done, we can extract the results. The predictions have been stored in alice’s storage. We can extract the value from the storage with read_value_from_storage.\n\ny_pred = runtime.read_value_from_storage(\"alice\", \"y_pred\")\n\n\nprint(f\"Plaintext Prediction: {y_pred}\")\nprint(f\"Moose Prediction: {y_pred}\")\n\nPlaintext Prediction: [[0.02581358 0.02598119 0.94782831]]\nMoose Prediction: [[0.02581358 0.02598119 0.94782831]]\n\n\nExcellent! As you can see Moose returns the same prediction as XGBoost. However, with Moose, we were able to compute the inference on the data owner’s data while keeping the data encrypted during the entire process!\nIf you want to learn about how to run Moose over the network with gRPC, you can check out this tutorial."
  },
  {
    "objectID": "posts/moose-hf-inference/moose_hf_blog.html#conclusion",
    "href": "posts/moose-hf-inference/moose_hf_blog.html#conclusion",
    "title": "Encrypted Inference with Moose and HuggingFace",
    "section": "Conclusion",
    "text": "Conclusion\nI hope that thanks to this tutorial you have a better idea of how you can perform encrypted inference with Moose. Thanks to libraries like Concrete-ML and Moose, we’re entering an exciting time where data scientists and machine learning engineers can maintain the confidentiality of sensitive datasets using encryption, without having to become experts in cryptography.\nThank you to the Moose team for this amazing contribution and reviewing this blog post."
  },
  {
    "objectID": "posts/moose-hf-inference/moose_hf_blog.html#resources",
    "href": "posts/moose-hf-inference/moose_hf_blog.html#resources",
    "title": "Encrypted Inference with Moose and HuggingFace",
    "section": "Resources:",
    "text": "Resources:\n\nSentiment Analysis on Encrypted Data with Homomorphic Encryption\nPrivate Deep Learning with MPC\nMoose library\nconcrete-ml library\nZama’s HuggingFace space"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "deep learning\n\n\nmpc\n\n\ncryptography\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nYann Dupis\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndeep learning\n\n\ncode\n\n\nfrom scratch\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nYann Dupis\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The goal of this blog is to explore my passion for machine learning, privacy, and other technical subjects.\nIn my free time, you can find me surfing at Ocean Beach (I grew up in a surf town in Southwestern France), playing jazz guitar, indoor rock climbing, and looking for a good pastrami on rye in the Bay Area."
  }
]