{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A General Purpose Deep Learning Architecture - Perceiver IO From Scratch\"\n",
    "toc: true\n",
    "format:\n",
    "    html: \n",
    "        code-fold: False\n",
    "jupyter: python3\n",
    "author: \"Yann Dupis\"\n",
    "date: \"2022-10-31\"\n",
    "categories: [deep learning, code, from scratch]\n",
    "image: \"./img/cross-attention.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent years, we saw tremendous progress across all the machine learning tasks from image classification, object detection, translation, question answering, text classification etc. Main drivers of this progress are more compute, data, new training approaches such as transfer learning, but also more sophisticated neural network architectures. Fortunately these models are available at out fingertips. If you tackle a computer vision task, you can leverage the [ Pytorch Image Models (timm)](https://timm.fast.ai/#List-Models-with-Pretrained-Weights) library. As of now it contains more than 50 different type of architectures translating into 600 models which have [different trade offs](https://www.kaggle.com/code/jhoward/which-image-models-are-best) in terms of accuracy and inference time depending on the task. For NLP, you can use the popular [transformer library](https://github.com/huggingface/transformers) from HuggingFace which offers almost [80k models](https://huggingface.co/models) based on about 140 different type of architectures.\n",
    "\n",
    "Each of these neural network architectures built upon one another by the research community, were able to achieve better performance thanks to very carefully hand crafted [inductive bias](https://samiraabnar.github.io/articles/2020-05/recurrence) for each specific task. For example, CNNs used in computer vision models such as ResNet have a locality bias where they assume close pixels are related to each other. LSTMs which used to be popular for NLP tasks before Transformers appeared, have a sequential bias where they process each element of a sequence one after another. However because the inductive bias is hand crafted for a specific task, existing architectures are often not able to generalize across multiple modalities. The Perceiver IO ([Jaegle et al, 2021](https://arxiv.org/abs/2107.14795)) architecture released by DeepMind in 2021 aim to solve this challenge and demonstrate they can achieve strong results across a wide variety of single-modal and multi-modal tasks such as: GLUE language benchmark, predicting optical flow between images, image classification, multi-modal video + audio classification, audio-video-label multi-modal autoencoding etc. \n",
    "\n",
    "In this blog post, we will implement a Perceiver IO architecture from scratch and we will apply it to two different domains: image classification and text classification. In conjunction to this blog post, I highly recommend watching [Drew Jaegle's talk](https://www.youtube.com/watch?v=wTZ3o36lXoQ) who's the main author of the Perceiver IO paper, and also read [this HuggingFace's blog post](https://huggingface.co/blog/perceiver). These resources in addition to training several Perceiver models with the [transformers library](https://github.com/huggingface/transformers) inspired me to implement this architecture from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydupis/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "\n",
    "The Perceiver architecture is based on the Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/pdf/1706.03762.pdf)). As described in [Drew Jaegle's presentation](https://www.youtube.com/watch?v=wTZ3o36lXoQ), the main benefits of transformers are:\n",
    "\n",
    "- They don't make specific assumption about the domain.\n",
    "- General purpose inductive bias.\n",
    "- Position is a feature instead of being a constraint.\n",
    "- Mainly uses matmuls which are efficiently computed by GPU and TPU.\n",
    "\n",
    "The cons are: \n",
    "\n",
    "- Attention scales quadratically based on the input. \n",
    "- Multilayer perceptron (MLP) scales linearly based on the input. \n",
    "\n",
    "Transformers consist of: a self-attention module, multilayer perceptron (also called feed-forward layer), layer normalization and skip connections. But really the heart of the transformers are their self-attention module and that's where one of the main Perceiver's innovation has been introduced. So let's first implement it, understand why it scales quadratically and learn how the perceiver architecture addresses this challenge.\n",
    "\n",
    "<center><img src=\"./img/transformer-architecture.png\" width=\"150\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "To get an intuition about the self-attention mechanism, I highly recommend [Peter Bloem's blog post](https://peterbloem.nl/blog/transformers) or the natural language processing with transformers [book](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/). Later in this post, we will demonstrate how the perceiver can be applied to images. But let's assume for now that our input is a sentence where each words of the sentence are represented by token embeddings. \n",
    "\n",
    "The self attention mechanism consists of the following steps:\n",
    "\n",
    "- Project the input (token embeddings in our case) into a query, key and values\n",
    "- Compute the similarity between the query and the key using a dot product. The result of the dot product represents the attention scores.\n",
    "- Multiply the attention scores by a scaling factor to normalize their variance and apply a softmax so each column of the matrix, representing the attention weights, sum up to 1.\n",
    "- Compute the dot product between the attention weights and the values.\n",
    "\n",
    "The main purpose of the self-attention mechanism is to produce a new representation of the token embeddings where now each new embeddings is a linear combination (or weighted average) of the ordinal embeddings. This allow us to encode some contextual information. For example, we have the following sentence: \"The tree bark is rough to the touch.\" and \"I hope her dog doesn’t bark when I knock on the door.\" In the two sentences, we have the word bark but with a different meaning. In the first sentence, bark refers to the tree and in the second sentence it refers to the dog. When applying the self attention mechanism to the embedding bark, the new embedding should give more weights to the word tree in the first sentence and dog in the second one in order to encode this context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        n_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(input_dim, n_channels)\n",
    "        self.k = nn.Linear(input_dim, n_channels)\n",
    "        self.v = nn.Linear(input_dim, n_channels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # (N, input_dim) . (input_dim, qk_channels) -> (N, qk_channels)\n",
    "        query = self.q(input)\n",
    "        # (N, input_dim) . (input_dim, qk_channels) -> (N, qk_channels)\n",
    "        key = self.k(input)\n",
    "        # (N, input_dim) . (input_dim, v_channels) -> (N, v_channels)\n",
    "        value = self.v(input)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(query.size(-1))\n",
    "        # (N, qk_channels) . (qk_channels, N) -> (N, N)\n",
    "        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n",
    "        print(f\"Attention score shape: {scores.shape}\")\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # (N, N) . (N, v_channels) -> (N, v_channels)\n",
    "        return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the self-attention module to a tensor of shape (1, 3, 5). We will assume the input is a sentence containing three words, each represented by an embedding token of length 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 5])\n",
      "Attention score shape: torch.Size([1, 3, 3])\n",
      "Output shape torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "x_embed = torch.ones(1, 3, 5)\n",
    "print(f\"Input shape: {x_embed.shape}\")\n",
    "self_attn = SelfAttention(5, 3)\n",
    "attn_out = self_attn(x_embed)\n",
    "print(f\"Output shape {attn_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the attention score has a shape of (1, 3, 3) and the output shape is (1, 3, 5). Let's now apply the self-attention mechanism to an input containing 10 words and see how it impacts the size of attention score tensor and the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10, 5])\n",
      "Attention score shape: torch.Size([1, 10, 10])\n",
      "Output shape torch.Size([1, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 10, 5)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "self_attn = SelfAttention(5, 3)\n",
    "attn_out = self_attn(x)\n",
    "print(f\"Output shape {attn_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an input of shape (1, 10, 5), the new attention score shape is (1, 10, 10). So as you can see the attention score scale quadratically based on the input. Same thing happen for the second dot product between the weights and the values. And the output shape is (1, 10, 3), so it increases linearly. Later we will see that the output of the self-attention module is fed into an MLP layer. Because the output of the self-attention module scales linearly based on the input, the MLP layer will also scale linearly based on the input.\n",
    "\n",
    "Ok, so let's learn how Perceiver IO is addressing the transformer scalability issue while maintaining its generality property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Attention\n",
    "\n",
    "Perceiver IO introduces the idea of using a cross attention module to encode the input into a latent space. To do so, instead of passing the input to the query, they use a latent variable which has smaller dimension than the input. You can think about this a latent variable as the learned initial stated for RNN.\n",
    "\n",
    "<center><img src=\"./img/cross-attention.png\" width=\"400\" /></center>\n",
    "\n",
    "You can find other examples of cross-attention for computer vision in the papers: End-to-End Object Detection with Transformers ([Carion et al, 2020](https://arxiv.org/abs/2005.12872)) and Object-Centric Learning with Slot Attention ([Locatello et al, 2020](https://arxiv.org/abs/2006.15055))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_kv_dim,\n",
    "        input_q_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(input_q_dim, qk_channels)\n",
    "        self.k = nn.Linear(input_kv_dim, qk_channels)\n",
    "        self.v = nn.Linear(input_kv_dim, v_channels)\n",
    "\n",
    "    def forward(self, input_kv, input_q):\n",
    "        # (M, input_q_dim) . (input_q_dim, qk_channels) -> (N, qk_channels)\n",
    "        query = self.q(input_q)\n",
    "        # (N, input_kv_dim) . (input_kv_dim, qk_channels) -> (N, qk_channels)\n",
    "        key = self.k(input_kv)\n",
    "        # (N, input_kv_dim) . (input_kv_dim, v_channels) -> (N, v_channels)\n",
    "        value = self.v(input_kv)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(query.size(-1))\n",
    "        # (M, qk_channels) . (qk_channels, N) -> (M, N)\n",
    "        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n",
    "        print(f\"Attention score shape: {scores.shape}\")\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        # (M, N) . (N, v_channels) -> (M, v_channels)\n",
    "        return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate it's cheaper to encode the input to a latent space instead of using self-attention, let's go through concrete examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 10, 5])\n",
      "Latent shape: {latent.shape}\n",
      "Attention score shape: torch.Size([1, 2, 10])\n",
      "Output shape torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1, 10, 5)\n",
    "latent = nn.Parameter(torch.randn(1, 2, 3))\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(\"Latent shape: {latent.shape}\")\n",
    "self_attn = CrossAttention(5, 3, 4, 3)\n",
    "attn_out = self_attn(x, latent)\n",
    "print(f\"Output shape {attn_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have our previous input of shape (1, 10, 5) and a new latent variable of shape (1, 2, 3). The attention score has now a shape of (1, 2, 10). So instead of the scaling quadratically based on the input size, the score scale linearly based on the size of the latent variable which can be controlled. The second matrix of the cross-attention module also scale linearly. But more importantly the output which will be used by the MLP layer also scales linearly based on the latent size instead of the input size. The output has now a shape of (1, 2, 3) instead of (1, 10, 3). \n",
    "\n",
    "Excellent! We now have implemented a cross-attention module which scales linearly based on the latent variable which will allow us to compute on larger inputs such as images or longer text sequences. Later, you will see that the Pereiver IO architecture also uses a self-attention module on the latent array. So let's create a general Attention module which can be parametrized to become a self-attention or cross-attention module based on our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_cross_attention,\n",
    "        input_kv_dim,\n",
    "        input_q_dim,\n",
    "        qk_channels_per_head,\n",
    "        v_channels_per_head,\n",
    "        attention_prob_dropout_prob=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "        if not is_cross_attention:\n",
    "            input_kv_dim = input_q_dim\n",
    "        \n",
    "        self.q = nn.Linear(input_q_dim, qk_channels_per_head)\n",
    "        self.k = nn.Linear(input_kv_dim, qk_channels_per_head)\n",
    "        self.v = nn.Linear(input_kv_dim, v_channels_per_head)\n",
    "        self.dropout = nn.Dropout(attention_prob_dropout_prob)\n",
    "\n",
    "    def forward(self, input_kv, input_q):\n",
    "        query = self.q(input_q)\n",
    "        \n",
    "        if self.is_cross_attention & (input_kv is not None):\n",
    "            key = self.k(input_kv)\n",
    "            value = self.v(input_kv)\n",
    "        else:\n",
    "            key = self.k(input_q)\n",
    "            value = self.v(input_q)\n",
    "\n",
    "        scale = 1.0 / math.sqrt(query.size(-1))\n",
    "        scores = torch.bmm(query, key.transpose(-1, -2)) * scale\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now we have implemented a single head attention but most transformers multi-head attention. The benefits of having multiple heads, it's that each head can focus on different aspects of an image (edges, colors, etc.) or a sentence instead of a single aspect. \n",
    "\n",
    "We can simply create multi-head attention layer by instantiating several heads and concatenating their outputs. Usually we also apply a linear layer to its final output. Note that it's possible to avoid instantiating an `AttentionHead` for each head and concatenating their output. It's possible instead to have the linear layers with number of output features equal to `number of channels per head * number of heads` for the query, key and the value. Then reshape the output from the query, key and value to `(batch_size, num_heads, time (N or M), number of channels per head)`. As an example, you can check the [transformers implementation](https://github.com/huggingface/transformers/blob/bd469c40659ce76c81f69c7726759d249b4aef49/src/transformers/models/perceiver/modeling_perceiver.py#L259)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_cross_attention,\n",
    "        input_kv_dim,\n",
    "        input_q_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_heads,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        qk_channels_per_head = qk_channels // num_heads\n",
    "        v_channels_per_head = v_channels // num_heads\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(\n",
    "                    is_cross_attention,\n",
    "                    input_kv_dim,\n",
    "                    input_q_dim,\n",
    "                    qk_channels_per_head,\n",
    "                    v_channels_per_head,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(v_channels, v_channels)\n",
    "\n",
    "    def forward(self, input, latent_embedding):\n",
    "        x = torch.cat([h(input, latent_embedding) for h in self.heads], dim=-1)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention input shape: torch.Size([1, 10, 5])\n",
      "Multi-head attention output shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "self_attention = MultiHeadAttention(True, 5, 3, 8, 6, 2)\n",
    "self_attention_multi_head = self_attention(x, latent)\n",
    "print(f\"Multi-head attention input shape: {x.shape}\")\n",
    "print(f\"Multi-head attention output shape: {self_attention_multi_head.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, we have our multi-head attention layer which can become a self-attention or cross-attention by setting the `is_cross_attention` parameter. Since we have implemented the main building block, we are ready to implement the full perceiver architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceiver Architecture\n",
    "\n",
    "The perceiver architecture consists of three main build blocks: encoder, processor and decoder. The input gets first encoded into a latent array, then the latent representation gets refined via several processing layers. Finally, the latent gets decoded into an output. As you can see on the diagram below, the encoder and decoder are using a cross-attention module, and the processor is using a self-attention module. WHat's amazing with the perceiver architecture is that it can handle any modalities thanks to the encoder and decoder modules. And the size of the inputs and outputs is not a problem anymore since they both use a cross-attention module where the latent size is independent of the input size. \n",
    "\n",
    "<center><img src=\"./img/perceiver-architecture.png\" width=\"400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the encoder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "The Perceiver encoder module is very similar to the transformer encoder you could find for in Bert except it's using cross-attention with a latent variable. In addition to the cross-attention module we just need to add:\n",
    "\n",
    "- A multi-perceptron module: two fully connected layer processing each latent vector independently, a GELU activation and dropout layer.\n",
    "- Two layer normalization layers  \n",
    "- Two skip connections\n",
    "\n",
    "Ok let's see, how we can combine these components to build our encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, widening_factor, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.Linear(input_size, input_size * widening_factor)\n",
    "        self.dense2 = nn.Linear(input_size * widening_factor, input_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class PerceiverEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        latent_embedding_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_heads,\n",
    "        widening_factor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(input_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(latent_embedding_dim)\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            is_cross_attention=True,\n",
    "            input_kv_dim=input_dim,\n",
    "            input_q_dim=latent_embedding_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(v_channels, widening_factor=widening_factor)\n",
    "\n",
    "    def forward(self, input, latent):\n",
    "        input_norm = self.layer_norm_1(input)\n",
    "        latent_embedding_norm = self.layer_norm_2(latent)\n",
    "        x_qkv = self.attention(input_norm, latent_embedding_norm)\n",
    "        x_qkv = x_qkv + latent\n",
    "        x_qkv = x_qkv + self.mlp(latent_embedding_norm)\n",
    "        return x_qkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 6])\n",
      "Latent variable shape: torch.Size([1, 2, 6])\n",
      "Encoder output shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "perceiver_encoded = PerceiverEncoder(6, 6, 8, 6, 2, 2)\n",
    "input = torch.ones(size=(1, 3, 6))\n",
    "latent = nn.Parameter(torch.randn(1, 2, 6))\n",
    "encoder_output = perceiver_encoded(input, latent)\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Latent variable shape: {latent.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we have implemented successfully our encoder layer. In fact, the processor and the decoder use the exact same ingredients, except the processor will use a self-attention. So let's refactor our encoder to use PerceiverLayer which can be parametrized to use a cross-attention or self-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_kv_dim,\n",
    "        input_q_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_heads,\n",
    "        widening_factor,\n",
    "        is_cross_attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if input_kv_dim is None:\n",
    "            input_kv_dim = input_q_dim\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(input_kv_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(input_q_dim)\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            is_cross_attention=is_cross_attention,\n",
    "            input_kv_dim=input_kv_dim,\n",
    "            input_q_dim=input_q_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = MLP(v_channels, widening_factor=widening_factor)\n",
    "\n",
    "    def forward(self, input_kv, input_q):\n",
    "        input_kv_norm = self.layer_norm_1(input_kv)\n",
    "        input_q_norm = self.layer_norm_2(input_q)\n",
    "        x_qkv = self.attention(input_kv_norm, input_q_norm)\n",
    "        x_qkv = x_qkv + input_q\n",
    "        x_qkv = x_qkv + self.mlp(input_q_norm)\n",
    "        return x_qkv\n",
    "\n",
    "\n",
    "class PerceiverEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        latent_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_heads,\n",
    "        widening_factor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = PerceiverLayer(\n",
    "            input_kv_dim=input_dim,\n",
    "            input_q_dim=latent_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "            widening_factor=widening_factor,\n",
    "            is_cross_attention=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input, latent_embeddings):\n",
    "        return self.encoder(input_kv=input, input_q=latent_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 6])\n",
      "Latent variable shape: torch.Size([1, 2, 6])\n",
      "Encoder output shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "perceiver_encoded = PerceiverEncoder(6, 6, 8, 6, 2, 2)\n",
    "input = torch.ones(size=(1, 3, 6))\n",
    "latent = nn.Parameter(torch.randn(1, 2, 6))\n",
    "perceiver_encoded(input, latent).shape\n",
    "\n",
    "encoder_output = perceiver_encoded(input, latent)\n",
    "print(f\"Input shape: {input.shape}\")\n",
    "print(f\"Latent variable shape: {latent.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our PerceiverEncoder layer now looks much simpler with our new generic PerceiverLayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "\n",
    "Our Processor layer is going to be responsible of refining the latent representation we obtained from the encoder. So this layer has a single input which is the latent array and we apply a self-attention module in conjunction with an MLP layer, layer normalization and skip connection (i.e `PerceiverLayer` configured as self-attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverProcessor(nn.Module):\n",
    "    def __init__(self, latent_dim, qk_channels, v_channels, num_heads, widening_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.processor = PerceiverLayer(\n",
    "            input_kv_dim=None,\n",
    "            input_q_dim=latent_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "            widening_factor=widening_factor,\n",
    "            is_cross_attention=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, latent):\n",
    "        return self.processor(input_kv=latent, input_q=latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent array (input) shape: torch.Size([1, 2, 6])\n",
      "Processor output shape: torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "latent = nn.Parameter(torch.randn(1, 2, 6))\n",
    "perceiver_processor = PerceiverProcessor(6, 8, 6, 2, 2)\n",
    "processor_output = perceiver_processor(latent)\n",
    "\n",
    "print(f\"Latent array (input) shape: {latent.shape}\")\n",
    "print(f\"Processor output shape: {processor_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to combine everything to have a complete architecture. We just need implement the decoder module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The perceiver decoder will map the latent array to an output array. To do so, we can query the latent array with a query vector. Note that's this query vector can be hand-designed, or learned embeddings or a function of the input. For this blog post, we will use learned embeddings. To query the latent array returned by the processor, we simply need to compute the cross attention between a learned query variable and the latent array. The query should have the same number of elements as the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_output_channels,\n",
    "        latent_dim,\n",
    "        query_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_heads,\n",
    "        widening_factor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder = PerceiverLayer(\n",
    "            input_kv_dim=latent_dim,\n",
    "            input_q_dim=query_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "            widening_factor=widening_factor,\n",
    "            is_cross_attention=True,\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Linear(query_dim, num_output_channels)\n",
    "\n",
    "    def forward(self, latent, query):\n",
    "        attn_output = self.decoder(latent, query)\n",
    "        logit = self.dense(attn_output)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the decoder with an expected output of 10 elements. For exmple, for an image classification task with 10 potential different label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent array shape: torch.Size([1, 3, 5])\n",
      "Query variable shape: torch.Size([1, 1, 10])\n",
      "Perceiver decoder output shape: torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "latent = torch.ones(1, 3, 5)\n",
    "query_variable = nn.Parameter(torch.randn(1, 1, 10))\n",
    "q_dim = 10\n",
    "kv_dim = 5\n",
    "num_output_channels = 10\n",
    "qk_channels = q_dim\n",
    "v_channels = qk_channels\n",
    "num_heads = 1\n",
    "\n",
    "perceiver_decoder = PerceiverDecoder(\n",
    "    num_output_channels, kv_dim, q_dim, qk_channels, v_channels, num_heads, 1\n",
    ")\n",
    "\n",
    "perceiver_decoder_output = perceiver_decoder(latent, query_variable)\n",
    "print(f\"Latent array shape: {latent.shape}\")\n",
    "print(f\"Query variable shape: {query_variable.shape}\")\n",
    "print(f\"Perceiver decoder output shape: {perceiver_decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the output of the decoder returns 10 elements matching the number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceiver IO = Encoder + Processor + Decoder\n",
    "\n",
    "We have now all the building block to create a complete PerceiverIO architecture. As discussed earlier, the latent variable for the encoder and the query variable for the decoder are learned. So we can create a `LearnedEmbedding` layer to instantiate a latent and query variables. Otherwise, it's straight forward. the encoder takes as an input the input array and the latent variable then returned a latent array. Then this latent array is fed into the processor. And finally the latent arrays is query by the decoder with a learned query. The output of our decoder will be the logits we will use to compute our loss when training the image and text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedEmbeddings(nn.Module):\n",
    "    def __init__(self, index_dim, num_channels=128):\n",
    "        super().__init__()\n",
    "        self.index_dim = index_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.learned_embeddings = nn.Parameter(torch.randn(index_dim, num_channels))\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        return self.learned_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "\n",
    "class PerceiverIO(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_labels,\n",
    "        input_dim,\n",
    "        latent_embedding_dim,\n",
    "        query_dim,\n",
    "        qk_channels,\n",
    "        v_channels,\n",
    "        num_latents,\n",
    "        num_heads,\n",
    "        nb_processor,\n",
    "        widening_factor,\n",
    "        input_processor=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_processor = input_processor if input_processor else nn.Identity()\n",
    "\n",
    "        self.latent_embeddings = LearnedEmbeddings(num_latents, latent_embedding_dim)\n",
    "        self.query_embeddings = LearnedEmbeddings(1, query_dim)\n",
    "\n",
    "        self.encoder = PerceiverEncoder(\n",
    "            input_dim=input_dim,\n",
    "            latent_dim=latent_embedding_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "            widening_factor=widening_factor,\n",
    "        )\n",
    "\n",
    "        self.processors = nn.ModuleList(\n",
    "            [\n",
    "                PerceiverProcessor(\n",
    "                latent_dim=latent_embedding_dim,\n",
    "                qk_channels=qk_channels,\n",
    "                v_channels=v_channels,\n",
    "                num_heads=num_heads,\n",
    "                widening_factor=widening_factor,\n",
    "            )\n",
    "                for _ in range(nb_processor)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = PerceiverDecoder(\n",
    "            num_output_channels=n_labels,\n",
    "            latent_dim=latent_embedding_dim,\n",
    "            query_dim=query_dim,\n",
    "            qk_channels=qk_channels,\n",
    "            v_channels=v_channels,\n",
    "            num_heads=num_heads,\n",
    "            widening_factor=widening_factor,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        latent_embeddings = self.latent_embeddings(batch_size)\n",
    "        query_embeddings = self.query_embeddings(batch_size)\n",
    "\n",
    "        inputs = self.input_processor(inputs)\n",
    "        x = self.encoder(inputs, latent_embeddings)\n",
    "\n",
    "        for p in self.processors:\n",
    "            x = p(x)\n",
    "\n",
    "        logits = self.decoder(x, query_embeddings)\n",
    "\n",
    "        return logits[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, we have now our full Perceiver IO architecture. Let's now dive into how in the Perceiver IO paper they pre-process an image or text to obtain the input arrays which will be fed into the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification\n",
    "\n",
    "### Pre-Processing\n",
    "\n",
    "To pre-process an image, in the Perceiver IO paper, they use different techniques involving 2D Fourier position embeddings with 2D convolution or flatten pixel values. But for this example, we will use another approach used in the paper where they flatten the pixels by applying a 1D convolution and add learned absolute positional 1D position embeddings. Based on the paper, the other approaches provide better results, but what's unique with this pre-processing approach it's that it doesn't provide any information about the 2D image structure.\n",
    "\n",
    "Let's assume we have a tensor representing a batch of images of shape (batch_size, 3, 32, 32). So each image have a width and height of 32 and 3 channels. To pre-process this image we will:\n",
    "\n",
    "- Apply a Conv1D layer to increase the number of channels, for example to 256. So now, let's say our output has a shape of (batch_size, 256, 32, 32)\n",
    "- The Conv1D will return a tensor with channel first. We want to make it channel last. Our new shape is (batch_size, 32, 32, 256)\n",
    "- We flatten the height with the width of the image so now we have a shape of (batch_size, 1024, 256)\n",
    "- We instantiate a trainable 1D position embedding for each pixel with for example 256 channels. So the shape of the embedding is (batch_size, 1024, 256)\n",
    "- We concatenate the output of the Conv1D layer with the trainable 1D position embedding based on the last dimension. So the final shape will be (batch_size, 1024, 512) \n",
    "\n",
    "That's it! Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, index_dim, num_channels=128):\n",
    "        super().__init__()\n",
    "        self.index_dim = index_dim\n",
    "        self.num_channels = num_channels\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(index_dim, num_channels))\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        return self.position_embeddings.expand(batch_size, -1, -1)\n",
    "\n",
    "\n",
    "class ImagePreProcessor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        spatial_downsample,\n",
    "        position_encoding_index_dim,\n",
    "        position_encoding_out_channels,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.spatial_downsample = spatial_downsample\n",
    "        self.postion_encoding_index_dim = position_encoding_index_dim\n",
    "\n",
    "        self.conv1d = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(spatial_downsample, spatial_downsample),\n",
    "        )\n",
    "\n",
    "        self.pos_enc = PositionalEncoding(\n",
    "            position_encoding_index_dim, num_channels=position_encoding_out_channels\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.shape[0]\n",
    "        # Increase the number of channels while keeping same height and width\n",
    "        inputs_post_conv1d = self.conv1d(inputs)\n",
    "        # Make channel last\n",
    "        inputs = torch.moveaxis(inputs, 1, -1)\n",
    "        # Flatten from (batch_size, img_size, img_size, num_channels) to (batch_size, img_size*img_size, num_channels)\n",
    "        inputs_post_conv1d = torch.reshape(\n",
    "            inputs_post_conv1d, [batch_size, np.prod(inputs.shape[1:-1]), -1]\n",
    "        )\n",
    "        # Instantiate learned 1D positional embeddings\n",
    "        pos_encoded = self.pos_enc(batch_size)\n",
    "        # Concat inputs post conv1d with 1D positional embeddings\n",
    "        return torch.cat([inputs_post_conv1d, pos_encoded], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate we get the expected output shape using an image of shape (3, 32, 32) with Conv1D output channels of 256 and a 1D positional embeddings with 256 channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 3, 32, 32])\n",
      "Processed image shape: torch.Size([1, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1, 3, 32, 32))\n",
    "image_processor = ImagePreProcessor(3, 256, 1, 32**2, 256)\n",
    "processed_image = image_processor(x)\n",
    "\n",
    "print(f\"Image shape: {x.shape}\")\n",
    "print(f\"Processed image shape: {processed_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, as expected our tensor has a shape of (1, 1024, 512). That's all we need to pre-process our images.\n",
    "\n",
    "We can finally train an image classification model using the Perceiver architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To demonstrate we can use the Perceiver IO architecture to classify images, we will use the MNIST dataset. Here, we are using a simple dataset to show the generality of the architecture. But note in the paper they were able to able to achieve strong results on the ImageNet dataset especially when the model has been pre-trained and the pre-processor is using 2D convolution and MaxPooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ydupis/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/ydupis/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = \"cuda\" if torch.has_cuda else \"cpu\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "eval_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=128, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_data, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "img_size = 28\n",
    "img_channels = 1\n",
    "img_processor_output_channels = 32\n",
    "img_processor_pos_encoding_out_channels = 32\n",
    "n_labels = 10\n",
    "input_dim = 64\n",
    "latent_embedding_dim = 128\n",
    "num_latents = 258\n",
    "query_dim = 128\n",
    "qk_channels = query_dim\n",
    "v_channels = qk_channels\n",
    "num_heads = 1\n",
    "nb_processor = 1\n",
    "widening_factor = 1\n",
    "spatial_downsample = 1\n",
    "\n",
    "\n",
    "image_processor = ImagePreProcessor(\n",
    "    img_channels,\n",
    "    img_processor_output_channels,\n",
    "    spatial_downsample,\n",
    "    img_size**2,\n",
    "    img_processor_pos_encoding_out_channels,\n",
    ")\n",
    "\n",
    "model = PerceiverIO(\n",
    "    n_labels,\n",
    "    input_dim,\n",
    "    latent_embedding_dim,\n",
    "    query_dim,\n",
    "    qk_channels,\n",
    "    v_channels,\n",
    "    num_latents,\n",
    "    num_heads,\n",
    "    nb_processor,\n",
    "    widening_factor,\n",
    "    input_processor=image_processor,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a generic training and evaluation loop we can reuse later to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, device, train_loader, optimizer, epoch, log_interval=50):\n",
    "    model.train()\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = logits.argmax(-1).cpu().numpy()\n",
    "        acc = accuracy_score(y_true=y.cpu().numpy(), y_pred=pred)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(x)}/{len(train_loader.dataset)} ({(100. * batch_idx  * len(x) / len(train_loader.dataset)):.2f}%)]\\t\"\n",
    "                f\"Loss: {loss.item():.2f} - Accuracy: {acc:.2f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def eval(model, loss_fn, device, eval_loader):\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in eval_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            eval_loss += loss_fn(logits, y).item()\n",
    "            pred = logits.argmax(-1, keepdim=True)\n",
    "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    eval_loss /= len(eval_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {eval_loss:.2f}, Accuracy: \"\n",
    "        f\"{(100. * correct/len(eval_loader.dataset)):.2f}%\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traning time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0.00%)]\tLoss: 2.36 - Accuracy: 0.13\n",
      "Train Epoch: 0 [12800/60000 (21.33%)]\tLoss: 1.29 - Accuracy: 0.49\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb Cell 53\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     train(model, loss_fn, device, train_dataloader, optimizer, e, log_interval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39meval\u001b[39m(model, loss_fn, device, eval_dataloader)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb Cell 53\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_fn, device, train_loader, optimizer, epoch, log_interval)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, y)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m pred \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.4.26/home/ydupis/Documents/datascience/yanndupis.github.io/posts/perceiver-io-experiment/perceiver_io_from_scratch.ipynb#Y103sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m acc \u001b[39m=\u001b[39m accuracy_score(y_true\u001b[39m=\u001b[39my\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), y_pred\u001b[39m=\u001b[39mpred)\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     adamw(params_with_grad,\n\u001b[1;32m    162\u001b[0m           grads,\n\u001b[1;32m    163\u001b[0m           exp_avgs,\n\u001b[1;32m    164\u001b[0m           exp_avg_sqs,\n\u001b[1;32m    165\u001b[0m           max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m           state_steps,\n\u001b[1;32m    167\u001b[0m           amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    168\u001b[0m           beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    169\u001b[0m           beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    170\u001b[0m           lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m           weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    172\u001b[0m           eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    173\u001b[0m           maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    174\u001b[0m           foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    175\u001b[0m           capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m func(params,\n\u001b[1;32m    219\u001b[0m      grads,\n\u001b[1;32m    220\u001b[0m      exp_avgs,\n\u001b[1;32m    221\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    222\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    223\u001b[0m      state_steps,\n\u001b[1;32m    224\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    225\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    226\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    228\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    229\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    230\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    231\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Documents/datascience/yanndupis.github.io/venv/lib/python3.8/site-packages/torch/optim/adamw.py:311\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    309\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    313\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=4e-3, weight_decay=1e-1)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    train(model, loss_fn, device, train_dataloader, optimizer, e, log_interval=100)\n",
    "    eval(model, loss_fn, device, eval_dataloader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model we acheived X accuracy. If we were spedind more time on tuning the model, use a 2D Fourier or 2D convolutional pre-processor it's very likely we could achieve a higher accuracy. But nonetheless we were able to classify images with the Perceiver model with zero information about the 2D structure of the images.\n",
    "\n",
    "Let's now explore how we can use the same architecture for a text classification task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "### Prep-Processing\n",
    "\n",
    "For this example, we will use the [AG_NEWS](https://pytorch.org/text/stable/datasets.html#ag-news) dataset which contains a corpus of news articles. Each article is classified as one of the following category: World, Sports, Business or Sci/Tech.\n",
    "\n",
    "With Perceiver IO, tokenization is extremely simple, you just need to convert the string to raw UTF-8 bytes. You don't need to apply more sophisticated tokenizer such as WordPeice or SentencePiece or BPE etc. Then for the numericalization process, we will just convert each byte to a byte ID. And finally, we will pad the sequence to a max sequence length. Perceiver IO aim to get rid of tokenizers is that they tend to perform less well on rare words and they don't transfer well from one language to another. \n",
    "\n",
    "In the paper implementation, they have some reserved tokens such as [BOS], [EOS], [SEP] etc. to represent the beginning of the sentence, end of the sentence but for simplicity, we will just convert the sentence to raw bytes then ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytesTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def to_int(self, inputs):\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = inputs.encode(\"utf-8\")\n",
    "        encoded = torch.frombuffer(inputs, dtype=torch.uint8).to(torch.int32)\n",
    "        return encoded.to(torch.int32)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can validate the tokenizer is working properly with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Hello Hello\n",
      "Tokenized sentence: tensor([ 72, 101, 108, 108, 111,  32,  72, 101, 108, 108, 111],\n",
      "       dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30071/1393707225.py:8: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:1111.)\n",
      "  encoded = torch.frombuffer(inputs, dtype=torch.uint8).to(torch.int32)\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello Hello\"\n",
    "tokenizer = BytesTokenizer()\n",
    "tokenized_input = tokenizer.to_int(input)\n",
    "print(f\"Sentence: {input}\")\n",
    "print(f\"Tokenized sentence: {tokenized_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of tokens matches the number of characters in the original sentence and also the world Hello is repeating twice.\n",
    "\n",
    "Next step is to implement a padding function so all the sequences in the batch will have the same length. If the length of the sequence exceeds the maximum sequence length, it will be truncated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(max_sequence_length, inputs, pad_value=0):\n",
    "    input_len = inputs.shape[1]\n",
    "    # Truncate sequence if exceeds max sequence length\n",
    "    if input_len > max_sequence_length:\n",
    "        inputs = inputs[:max_sequence_length]\n",
    "    # Pad sequence with pad value if shorter the max sequence length\n",
    "    pad_len = max_sequence_length - input_len\n",
    "    padded_input = torch.nn.functional.pad(inputs, pad=((0, pad_len)), value=pad_value)\n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenizer and padding function, we can create a collate_batch function. This function will be used by our `DataLoader` to tokenize and pad each sentence contained in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 256\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "\n",
    "    for (_label, _text) in batch:\n",
    "        # Convert labels [1, 2, 3, 4] to [0, 1, 2, 3] for loss function\n",
    "        label_processed = _label - 1\n",
    "        label_list.append(label_processed)\n",
    "        # Tokenize and numericalize sentence\n",
    "        tokenized_text = torch.unsqueeze(tokenizer.to_int(_text), 0)\n",
    "        # Pad and truncate the tokenized sentence to match MAX_SEQUENCE_LEN\n",
    "        padded_text = pad(MAX_SEQUENCE_LEN, tokenized_text)\n",
    "        text_list.append(padded_text)\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = torch.cat(text_list, dim=0)\n",
    "    return text_list.to(device), label_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are just missing one piece of the puzzle before starting to train our model which is a text processor layer. This processing layer is very similar to the one you could find in a Bert model. It consists of:\n",
    "\n",
    "- Convert each token in the sentence to embeddings\n",
    "- Represent also the position of each token as embeddings\n",
    "- Add the tokens embeddings to the function embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_position_embeddings=256):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embeddings(inputs)\n",
    "        seq_len = inputs.shape[1]\n",
    "        position_ids = torch.arange(0, seq_len, device=inputs.device)\n",
    "        embeddings = embeddings + self.position_embeddings(position_ids)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We can finally train our model to classify news articles. First we create a Dataloader for the training and evaluation datasets using the `collate_batch` function previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "eval_iter = AG_NEWS(split=\"test\")\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "eval_dataset = to_map_style_dataset(eval_iter)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=512, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=512, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_len = [len(train_dataset._data[i][1]) for i in range(1000)]\n",
    "# print(np.average(article_len))\n",
    "# print(article_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we instantiate the Perceiver model. It's the exact same model we used for the image classification task except we use a text processor layer, set the number of labels to 4 and adjust the input_dim parameter to the embedding length for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "n_labels = 4\n",
    "input_dim = MAX_SEQUENCE_LEN\n",
    "latent_embedding_dim = 64\n",
    "num_latents = 64\n",
    "query_dim = 64\n",
    "qk_channels = query_dim\n",
    "v_channels = qk_channels\n",
    "num_heads = 4\n",
    "nb_processor = 2\n",
    "widening_factor = 1\n",
    "\n",
    "\n",
    "text_processor = TextProcessor(vocab_size, input_dim)\n",
    "\n",
    "model = PerceiverIO(\n",
    "    n_labels,\n",
    "    input_dim,\n",
    "    latent_embedding_dim,\n",
    "    query_dim,\n",
    "    qk_channels,\n",
    "    v_channels,\n",
    "    num_latents,\n",
    "    num_heads,\n",
    "    nb_processor,\n",
    "    widening_factor,\n",
    "    input_processor=text_processor,\n",
    ")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we can reuse the training and evaluation loop for the previous tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/120000 (0.00%)]\tLoss: 1.55 - Accuracy: 0.22\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 57.39%\n",
      "\n",
      "Train Epoch: 1 [0/120000 (0.00%)]\tLoss: 1.01 - Accuracy: 0.59\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 60.36%\n",
      "\n",
      "Train Epoch: 2 [0/120000 (0.00%)]\tLoss: 0.94 - Accuracy: 0.62\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 62.58%\n",
      "\n",
      "Train Epoch: 3 [0/120000 (0.00%)]\tLoss: 0.90 - Accuracy: 0.63\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 62.66%\n",
      "\n",
      "Train Epoch: 4 [0/120000 (0.00%)]\tLoss: 0.88 - Accuracy: 0.64\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 64.55%\n",
      "\n",
      "Train Epoch: 5 [0/120000 (0.00%)]\tLoss: 0.86 - Accuracy: 0.65\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 66.28%\n",
      "\n",
      "Train Epoch: 6 [0/120000 (0.00%)]\tLoss: 0.76 - Accuracy: 0.70\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 67.17%\n",
      "\n",
      "Train Epoch: 7 [0/120000 (0.00%)]\tLoss: 0.81 - Accuracy: 0.67\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 65.99%\n",
      "\n",
      "Train Epoch: 8 [0/120000 (0.00%)]\tLoss: 0.79 - Accuracy: 0.69\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 68.34%\n",
      "\n",
      "Train Epoch: 9 [0/120000 (0.00%)]\tLoss: 0.71 - Accuracy: 0.73\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 68.51%\n",
      "\n",
      "Train Epoch: 10 [0/120000 (0.00%)]\tLoss: 0.73 - Accuracy: 0.73\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 68.74%\n",
      "\n",
      "Train Epoch: 11 [0/120000 (0.00%)]\tLoss: 0.75 - Accuracy: 0.71\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 69.01%\n",
      "\n",
      "Train Epoch: 12 [0/120000 (0.00%)]\tLoss: 0.72 - Accuracy: 0.69\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 68.51%\n",
      "\n",
      "Train Epoch: 13 [0/120000 (0.00%)]\tLoss: 0.72 - Accuracy: 0.72\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 69.01%\n",
      "\n",
      "Train Epoch: 14 [0/120000 (0.00%)]\tLoss: 0.70 - Accuracy: 0.73\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 69.21%\n",
      "\n",
      "Train Epoch: 15 [0/120000 (0.00%)]\tLoss: 0.68 - Accuracy: 0.75\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 69.80%\n",
      "\n",
      "Train Epoch: 16 [0/120000 (0.00%)]\tLoss: 0.70 - Accuracy: 0.73\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 69.96%\n",
      "\n",
      "Train Epoch: 17 [0/120000 (0.00%)]\tLoss: 0.67 - Accuracy: 0.72\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 70.45%\n",
      "\n",
      "Train Epoch: 18 [0/120000 (0.00%)]\tLoss: 0.62 - Accuracy: 0.75\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 70.47%\n",
      "\n",
      "Train Epoch: 19 [0/120000 (0.00%)]\tLoss: 0.66 - Accuracy: 0.75\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 70.50%\n",
      "\n",
      "Train Epoch: 20 [0/120000 (0.00%)]\tLoss: 0.66 - Accuracy: 0.73\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.17%\n",
      "\n",
      "Train Epoch: 21 [0/120000 (0.00%)]\tLoss: 0.62 - Accuracy: 0.75\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.07%\n",
      "\n",
      "Train Epoch: 22 [0/120000 (0.00%)]\tLoss: 0.64 - Accuracy: 0.76\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 70.91%\n",
      "\n",
      "Train Epoch: 23 [0/120000 (0.00%)]\tLoss: 0.60 - Accuracy: 0.78\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.04%\n",
      "\n",
      "Train Epoch: 24 [0/120000 (0.00%)]\tLoss: 0.57 - Accuracy: 0.78\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.79%\n",
      "\n",
      "Train Epoch: 25 [0/120000 (0.00%)]\tLoss: 0.59 - Accuracy: 0.77\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.50%\n",
      "\n",
      "Train Epoch: 26 [0/120000 (0.00%)]\tLoss: 0.55 - Accuracy: 0.81\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.08%\n",
      "\n",
      "Train Epoch: 27 [0/120000 (0.00%)]\tLoss: 0.57 - Accuracy: 0.77\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.99%\n",
      "\n",
      "Train Epoch: 28 [0/120000 (0.00%)]\tLoss: 0.57 - Accuracy: 0.77\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.96%\n",
      "\n",
      "Train Epoch: 29 [0/120000 (0.00%)]\tLoss: 0.52 - Accuracy: 0.79\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.30%\n",
      "\n",
      "Train Epoch: 30 [0/120000 (0.00%)]\tLoss: 0.55 - Accuracy: 0.77\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.93%\n",
      "\n",
      "Train Epoch: 31 [0/120000 (0.00%)]\tLoss: 0.54 - Accuracy: 0.80\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.97%\n",
      "\n",
      "Train Epoch: 32 [0/120000 (0.00%)]\tLoss: 0.53 - Accuracy: 0.79\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.34%\n",
      "\n",
      "Train Epoch: 33 [0/120000 (0.00%)]\tLoss: 0.49 - Accuracy: 0.81\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.30%\n",
      "\n",
      "Train Epoch: 34 [0/120000 (0.00%)]\tLoss: 0.51 - Accuracy: 0.80\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.22%\n",
      "\n",
      "Train Epoch: 35 [0/120000 (0.00%)]\tLoss: 0.44 - Accuracy: 0.83\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.34%\n",
      "\n",
      "Train Epoch: 36 [0/120000 (0.00%)]\tLoss: 0.49 - Accuracy: 0.81\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.04%\n",
      "\n",
      "Train Epoch: 37 [0/120000 (0.00%)]\tLoss: 0.46 - Accuracy: 0.84\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.92%\n",
      "\n",
      "Train Epoch: 38 [0/120000 (0.00%)]\tLoss: 0.44 - Accuracy: 0.84\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 71.71%\n",
      "\n",
      "Train Epoch: 39 [0/120000 (0.00%)]\tLoss: 0.52 - Accuracy: 0.79\n",
      "\n",
      "Test set: Average loss: 0.00, Accuracy: 72.61%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = AdamW(model.parameters(), lr=2e-3, weight_decay=1e-1)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-3)\n",
    "\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    train(model, loss_fn, device, train_dataloader, optimizer, e, log_interval=1000)\n",
    "    eval(model, loss_fn, device, eval_dataloader)\n",
    "    # scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "85% accuracy on the evaluation set, not bad. With the same architecture we were able to solve to distinc tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ressources\n",
    "\n",
    "- [Perceiver IO: A General Architecture for Structured Inputs & Outputs (Jaegle et al, 2021)](https://arxiv.org/abs/2107.14795)\n",
    "- [CS25 I Stanford Seminar - DeepMind's Perceiver and Perceiver IO: new data family architecture](https://www.youtube.com/watch?v=wTZ3o36lXoQ)\n",
    "- [Perceiver IO: a scalable, fully-attentional model that works on any modality](https://huggingface.co/blog/perceiver)\n",
    "- [Perceiver IO implementation from DeepMind](https://github.com/deepmind/deepmind-research/tree/master/perceiver)\n",
    "- [Perceiver IO implementation from HuggingFace Transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/perceiver)\n",
    "- [Natural Language Processing with Transformers book](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9e669cd6178fd56a41685652a859edb59342913a1af433e93f88bb610a036144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
