{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Encrypted Inference with Moose and HuggingFace\"\n",
    "toc: true\n",
    "format:\n",
    "    html: \n",
    "        code-fold: False\n",
    "jupyter: python3\n",
    "author: \"Yann Dupis\"\n",
    "date: \"2023-01-09\"\n",
    "categories: [deep learning, mpc, cryptography]\n",
    "image: \"./img/moose_cow_bay.jpeg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other day, I was very inspired by the blog post [Sentiment Analysis on Encrypted Data with Homomorphic Encryption](https://huggingface.co/blog/sentiment-analysis-fhe) co-written by Zama and HuggingFace. Zama has created an excellent encrypted machine learning library, [Concrete-ML](https://huggingface.co/blog/sentiment-analysis-fhe), based on fully homomorphic encryption (FHE). Concrete-ML enables data scientists to easily turn their machine learning models into an homomorphic equivalent in order to perform inference on encrypted data. In the blog post, the authors demonstrate how you can easily perform sentiment analysis on encrypted data with this library. As you can imagine, sometimes you will need to perform sentiment analysis on text containing sensitive information. With FHE, the data always remains encrypted during computation, which enables data scientists to provide a machine learning service to a user while maintaining data confidentiality.\n",
    "\n",
    "The last several years, I was very fortunate to also work at the intersection of machine learning and cryptography. One of my collaborations with Morten Dahl, Jason Mancuso, Dragos Roturu and Lex Verona that I am very excited about is [Moose](https://github.com/tf-encrypted/moose). Moose is a distributed dataflow framework for encrypted machine learning and data processing. Moose's cryptographic protocol is based on secure multi-party-computation (MPC). Depending on the scenario, FHE and MPC have different pros and cons. Currently MPC generally tends to be more performant, however the protocol requires 2 or 3 non-colluding parties (e.g a data owner and a data scientist) willing to perform computations together. If you want to learn about MPC in the context of machine learning, I highly recommend [this very comprehensive blog post](https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/) where Morten implements an MPC protocol from scratch for Deep Learning.\n",
    "\n",
    "In the rest of this blog post, I will show how you can perform encrypted inference with Moose using the sentiment analysis use case from Zama and HuggingFace's [blog post](https://huggingface.co/blog/sentiment-analysis-fhe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The sentiment analysis model will be trained on the [Twitter US Airline Sentiment dataset from Kaggle](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment). To train the model, we will use the code provided in the blog post. The sentiment model consists of a RoBERTa ([Liu et al, 2019](https://arxiv.org/abs/1907.11692)) transformer to extract features from the text, and an XGBoost model on top of it to classify the tweets into positive, negative, or neutral classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of positive examples: 16.14%\n",
      "Proportion of negative examples: 62.69%\n",
      "Proportion of neutral examples: 21.17%\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"local_datasets/twitter-airline-sentiment/Tweets.csv\"):\n",
    "    raise ValueError(\"Please launch the `download_data.sh` script to get datasets\")\n",
    "\n",
    "train = pd.read_csv(\"local_datasets/twitter-airline-sentiment/Tweets.csv\", index_col=0)\n",
    "text_X, y = train[\"text\"], train[\"airline_sentiment\"] \n",
    "y = y.replace([\"negative\", \"neutral\", \"positive\"], [0, 1, 2])\n",
    "\n",
    "pos_ratio = y.value_counts()[2] / y.value_counts().sum()\n",
    "neg_ratio = y.value_counts()[0] / y.value_counts().sum()\n",
    "neutral_ratio = y.value_counts()[1] / y.value_counts().sum()\n",
    "\n",
    "print(f\"Proportion of positive examples: {round(pos_ratio * 100, 2)}%\")\n",
    "print(f\"Proportion of negative examples: {round(neg_ratio * 100, 2)}%\")\n",
    "print(f\"Proportion of neutral examples: {round(neutral_ratio * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the tweets are classified into three categories: positive, negative and neutral.\n",
    "\n",
    "For the feature extractor, in the blog post, the authors use a [RoBerta transformer pre-trained on Tweets](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the tokenizer (converts text to tokens)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")\n",
    "\n",
    "# Load the pre-trained model\n",
    "transformer_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will be responsible for extracting the features from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that transforms a list of texts to their representation\n",
    "# learned by the transformer.\n",
    "def text_to_tensor(\n",
    "    list_text_X_train: list,\n",
    "    transformer_model: AutoModelForSequenceClassification,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    device: str,\n",
    ") -> np.ndarray:\n",
    "    # Tokenize each text in the list one by one\n",
    "    tokenized_text_X_train_split = []\n",
    "    for text_x_train in list_text_X_train:\n",
    "        tokenized_text_X_train_split.append(\n",
    "            tokenizer.encode(text_x_train, return_tensors=\"pt\")\n",
    "        )\n",
    "\n",
    "    # Send the model to the device\n",
    "    transformer_model = transformer_model.to(device)\n",
    "    output_hidden_states_list = []\n",
    "\n",
    "    for tokenized_x in tqdm.tqdm(tokenized_text_X_train_split):\n",
    "        # Pass the tokens through the transformer model and get the hidden states\n",
    "        # Only keep the last hidden layer state for now\n",
    "        output_hidden_states = transformer_model(\n",
    "            tokenized_x.to(device), output_hidden_states=True\n",
    "        )[1][-1]\n",
    "        # Average over the tokens axis to get a representation at the text level.\n",
    "        output_hidden_states = output_hidden_states.mean(dim=1)\n",
    "        output_hidden_states = output_hidden_states.detach().cpu().numpy()\n",
    "        output_hidden_states_list.append(output_hidden_states)\n",
    "\n",
    "    return np.concatenate(output_hidden_states_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the feature extractor on the training and testing set, then train the XGBoost model on the feature extractor's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13176/13176 [11:32<00:00, 19.02it/s]\n",
      "100%|██████████| 1464/1464 [01:17<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.844869459623558\n",
      "Best parameters: {'max_depth': 1, 'n_estimators': 50, 'n_jobs': -1}\n",
      "Accuracy: 0.8559\n",
      "Average precision score for positive class: 0.9015\n",
      "Average precision score for negative class: 0.9675\n",
      "Average precision score for neutral class: 0.7517\n"
     ]
    }
   ],
   "source": [
    "# Split in train test\n",
    "text_X_train, text_X_test, y_train, y_test = train_test_split(\n",
    "    text_X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Let's vectorize the text using the transformer\n",
    "list_text_X_train = text_X_train.tolist()\n",
    "list_text_X_test = text_X_test.tolist()\n",
    "\n",
    "X_train_transformer = text_to_tensor(\n",
    "    list_text_X_train, transformer_model, tokenizer, device\n",
    ")\n",
    "X_test_transformer = text_to_tensor(\n",
    "    list_text_X_test, transformer_model, tokenizer, device\n",
    ")\n",
    "\n",
    "# Let's build our model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# A gridsearch to find the best parameters\n",
    "parameters = {\n",
    "    \"max_depth\": [1],\n",
    "    \"n_estimators\": [10, 30, 50],\n",
    "    \"n_jobs\": [-1],\n",
    "}\n",
    "\n",
    "# Now we have a representation for each tweet, we can train a model on these.\n",
    "grid_search = GridSearchCV(model, parameters, cv=3, n_jobs=1, scoring=\"accuracy\")\n",
    "grid_search.fit(X_train_transformer, y_train)\n",
    "\n",
    "# Check the accuracy of the best model\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "\n",
    "# Check best hyperparameters\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Extract best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Compute the metrics for each class\n",
    "\n",
    "y_proba = best_model.predict_proba(X_test_transformer)\n",
    "\n",
    "# Compute the accuracy\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy_transformer_xgboost = np.mean(y_pred == y_test)\n",
    "print(f\"Accuracy: {accuracy_transformer_xgboost:.4f}\")\n",
    "\n",
    "y_pred_positive = y_proba[:, 2]\n",
    "y_pred_negative = y_proba[:, 0]\n",
    "y_pred_neutral = y_proba[:, 1]\n",
    "\n",
    "ap_positive_transformer_xgboost = average_precision_score(\n",
    "    (y_test == 2), y_pred_positive\n",
    ")\n",
    "ap_negative_transformer_xgboost = average_precision_score(\n",
    "    (y_test == 0), y_pred_negative\n",
    ")\n",
    "ap_neutral_transformer_xgboost = average_precision_score((y_test == 1), y_pred_neutral)\n",
    "\n",
    "print(\n",
    "    f\"Average precision score for positive class: \"\n",
    "    f\"{ap_positive_transformer_xgboost:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average precision score for negative class: \"\n",
    "    f\"{ap_negative_transformer_xgboost:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average precision score for neutral class: \"\n",
    "    f\"{ap_neutral_transformer_xgboost:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent, we have a sentiment analysis model with an 85% accuracy. We can run the model on a sample tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clear_proba [[0.02582786 0.02599407 0.94817805]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tested_tweet = [\"AirFrance is awesome, almost as much as Zama!\"]\n",
    "X_tested_tweet = text_to_tensor(tested_tweet, transformer_model, tokenizer, device)\n",
    "np.save(\"data/x_tested_tweet.npy\", X_tested_tweet)\n",
    "clear_proba = best_model.predict_proba(X_tested_tweet)\n",
    "print(f\"Proba prediction in plaintext {clear_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encrypted Inference with Moose\n",
    "\n",
    "Now that we have a model trained, we are ready to serve encrypted inference with Moose. For simplicity, we will start by locally prototyping this computation happening between the different parties using the `pm.LocalMooseRuntime`.\n",
    "\n",
    "To serve encrypted inference, we will have to perform the following steps:\n",
    "- Convert the trained model to [ONNX format](https://onnx.ai/).\n",
    "- Convert the model from ONNX to a Moose computation.\n",
    "- Run encrypted inference by evaluating the Moose computation.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxmltools.convert import convert_xgboost\n",
    "from skl2onnx.common import data_types as onnx_dtypes\n",
    "\n",
    "import pymoose as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to ONNX\n",
    "We can convert the XGBoost model into an ONNX proto using the `convert_xgboos` method from the [onnxmltools](https://github.com/onnx/onnxmltools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_test_transformer[0].shape[0]\n",
    "initial_type = (\"float_input\", onnx_dtypes.FloatTensorType([None, n_features]))\n",
    "onnx_proto = convert_xgboost(best_model, initial_types=[initial_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert ONNX to Moose Predictor\n",
    "\n",
    "PyMoose provides several predictor classes to translate an ONNX model into a PyMoose DSL program. Because the trained model is an XGBoost model, we can use the class `tree_ensemble.TreeEnsembleClassifier`. The class has a method `from_onnx` which will parse the ONNX file. The returned object is callable. When called, it will compute the forward pass of the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pm.predictors.TreeEnsembleClassifier.from_onnx(onnx_proto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Moose Computation\n",
    "\n",
    "To express this computation, Moose offers a Python DSL (internally referred to as the eDSL, i.e. \"embedded\" DSL). As you will notice, the syntax is very similar to the scientific computation library [Numpy](https://numpy.org/).\n",
    "\n",
    "The main difference is the notion of placements. There are two types of placements: host placement and replicated placement. With Moose, every operation under a host placement context is computed on plaintext values (not encrypted). Every operation under a replicated placement is performed on secret shared values (encrypted).\n",
    "\n",
    "We will compute the inference between three different players, each of them representing a host placement: a data owner, a data scientist, and a third party. The three players are grouped under the replicated placement to perform the encrypted computation. Currently, the MPC protocol of Moose expects three parties, but other MPC schemes can expect two parties. practice, the third party could be a secure enclave that the data scientist and data owner can't access.\n",
    "\n",
    "When we have instantiated the `pm.predictors.TreeEnsembleClassifier` class, under the hood three host placements have been instiated: alice, bob and carole. For our use case, alice will represent the data owner, bob the model owner and carole the third party.\n",
    "\n",
    "The Moose computation below performs the following steps:\n",
    "\n",
    "- Loads the tweet (after running the feature extractor) in plaintext from alice's (data owner) storage.\n",
    "- Secret share (encrypts) the data.\n",
    "- Computes XGBoost inference on secret shared data.\n",
    "- Reveals the prediction only to alice (the data owner) and saves it into its storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pm.computation\n",
    "def moose_predictor_computation():\n",
    "    # Alice (data owner) load their data in plaintext\n",
    "    # Then the data gets converted from float to fixed-point\n",
    "    with predictor.alice:\n",
    "        x = pm.load(\"x\", dtype=pm.float64)\n",
    "        x_fixed = pm.cast(x, dtype=pm.predictors.predictor_utils.DEFAULT_FIXED_DTYPE)\n",
    "    # The data gets secret shared when moving from host placement\n",
    "    # to replicated placement.\n",
    "    # Then compute the logistic regression on secret shared data\n",
    "    with predictor.replicated:\n",
    "        y_pred = predictor(x_fixed, pm.predictors.predictor_utils.DEFAULT_FIXED_DTYPE)\n",
    "\n",
    "    # The predictions gets revealed only to Alice (the data owner)\n",
    "    # Convert the data from fixed-point to floats and save the data in the storage\n",
    "    with predictor.alice:\n",
    "        y_pred = pm.cast(y_pred, dtype=pm.float64)\n",
    "        y_pred = pm.save(\"y_pred\", y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the computation\n",
    "For simplicity, we will use `pm.LocalMooseRuntime` to locally simulate this computation running across hosts. To do so, we need to provide: the Moose computation, the list of host identities to simulate, and a mapping of the data stored by each simulated host.\n",
    "\n",
    "Since the data owner is represented by alice, we will place the patients' data in alice's storage.\n",
    "\n",
    "Once you have instantiated the `pm.LocalMooseRuntime` with the identities and additional storage mapping and the runtime set as default, you can simply call the Moose computation to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "executive_storage = {\n",
    "    \"alice\": {\"x\": X_tested_tweet.astype(np.float64)},\n",
    "    \"bob\": {},\n",
    "    \"carole\": {},\n",
    "}\n",
    "identities = [plc.name for plc in predictor.host_placements]\n",
    "\n",
    "runtime = pm.LocalMooseRuntime(identities, storage_mapping=executive_storage)\n",
    "runtime.set_default()\n",
    "\n",
    "_ = moose_predictor_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the computation is done, we can extract the results. The predictions have been stored in alice's storage. We can extract the value from the storage with `read_value_from_storage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = runtime.read_value_from_storage(\"alice\", \"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaintext Prediction: [[0.02581358 0.02598119 0.94782831]]\n",
      "Moose Prediction: [[0.02581358 0.02598119 0.94782831]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Plaintext Prediction: {y_pred}\")\n",
    "print(f\"Moose Prediction: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! As you can see Moose returns the same prediction as XGBoost. However, with Moose, we were able to compute the inference on the data owner's data while keeping the data encrypted during the entire process!\n",
    "\n",
    "If you want to learn about how to run Moose over the network with gRPC, you can [check out this tutorial](https://github.com/tf-encrypted/moose/blob/main/tutorials/ml-inference-with-onnx.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope that thanks to this tutorial you have a better idea of how you can perform encrypted inference with Moose. Thanks to libraries like Concrete-ML and Moose, we're entering an exciting time where data scientists and machine learning engineers can maintain the confidentiality of sensitive datasets using encryption, without having to become experts in cryptography.\n",
    "\n",
    "Thank you to the Moose team for this amazing contribution and reviewing this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "- [Sentiment Analysis on Encrypted Data with Homomorphic Encryption](https://huggingface.co/blog/sentiment-analysis-fhe)\n",
    "- [Private Deep Learning with MPC](https://mortendahl.github.io/2017/04/17/private-deep-learning-with-mpc/)\n",
    "- [Moose library](https://github.com/tf-encrypted/moose)\n",
    "- [concrete-ml library](https://github.com/zama-ai/concrete-ml)\n",
    "- [Zama's HuggingFace space](https://huggingface.co/zama-fhe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0226896e55f124ab26529a55091fe7354d49a3f395f7421b92d5b02660016830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
